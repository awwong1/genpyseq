{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Sequence Prediction Using Character Level Recurrent Networks\n",
    "\n",
    "We extend the approach outlined in `char_rnn_one_file_code_gen.ipynb` to improve efficiency and to accomodate other recurrent cells. Notably:\n",
    "\n",
    "* Training will use multiple files, instead of a single file\n",
    "* Validation sets will be introduced to avoid overfitting on our training data\n",
    "* We will have our model utilize mini-batches to speed up training\n",
    "\n",
    "Our outlined task is still the same.\n",
    "\n",
    "**Given a sequence of characters, predict the next likely character in the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "from json import load\n",
    "from math import floor\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We will be training on a set of 180 preprocessed Python files (and validating on a set of 20 other files) arbitrarily sampled from [GitHub BigQuery Python Extracts](https://bigquery.cloud.google.com/table/fh-bigquery:github_extracts.contents_py_201802snap?pli=1).\n",
    "\n",
    "We limit the characters that our neural network can produce to a subset of standard ASCII.\n",
    "* `ORD 2*, 3*, 9, 10, 32-126`\n",
    "  * `ORD 0` for padding (special, used in batch_size > 1 with variable length sequences)\n",
    "  * `ORD 2` for start of text (special, never predicted)\n",
    "  * `ORD 3` for end of text (special, prediction ends)\n",
    "  * `ORD 9` horizontal tab \"\\t\"\n",
    "  * `ORD 10` NL line feed, new line \"\\n\"\n",
    "  * `ORD 32-126` Space, Punctuation, Digits, English Letters\n",
    "\n",
    "NOTE: The full dataset contains files written using non standard characters. For the models in this notebook, we ensure that all Python files within our dataset are composed only of ASCII characters that we accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\\x00'\n",
      "2: '\\x02'\n",
      "3: '\\x03'\n",
      "9: '\\t'\n",
      "10: '\\n'\n",
      "32: ' '\n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Num Training Files: 180\n",
      "Num Validation Files: 20\n"
     ]
    }
   ],
   "source": [
    "# Possible Characters for neural network\n",
    "VALID_UNICODE_IDS = (0, 2, 3, 9, 10) + tuple(range(32, 127))\n",
    "for uid in VALID_UNICODE_IDS:\n",
    "    if uid <= 32:\n",
    "        print(\"{}: {}\".format(uid, repr(chr(uid))))\n",
    "        continue\n",
    "    print(chr(uid), end=\"\")\n",
    "print()\n",
    "\n",
    "# Special Characters\n",
    "PAD = chr(0)\n",
    "FILE_START = chr(2)\n",
    "FILE_END = chr(3)\n",
    "\n",
    "CHARACTERS = set(chr(id) for id in VALID_UNICODE_IDS)\n",
    "INT2CHAR = dict(enumerate(CHARACTERS))\n",
    "CHAR2INT = {char: idx for idx, char in INT2CHAR.items()}\n",
    "\n",
    "with open(\"./data/train.json\", \"r\") as f:\n",
    "    training_data = load(f)\n",
    "with open(\"./data/validate.json\", \"r\") as f:\n",
    "    validation_data = load(f)\n",
    "\n",
    "print(\"~\" * 25)\n",
    "print(\"Num Training Files: {}\".format(len(training_data)))\n",
    "print(\"Num Validation Files: {}\".format(len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly take a look at what is contained within our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002class dotdictify(dict):\n",
      "\n",
      "    def __init__(self, value=None):\n",
      "        if value is None:\n",
      "            pass\n",
      "        elif isinstance(value, dict):\n",
      "            for key in value:\n",
      "                self.__setitem__(key, value[key])\n",
      "        else:\n",
      "            raise TypeError('expected dict')\n",
      "\n",
      "    def __setitem__(self, key, value):\n",
      "        if '.' in key:\n",
      "            myKey, restOfKey = key.split('.', 1)\n",
      "            target = self.setdefault(myKey, dotdictify())\n",
      "            if not isinstance(target, dotdictify) and isinstance(target, dict):\n",
      "                target = dotdictify(target)\n",
      "            if not isinstance(target, dotdictify):\n",
      "                raise KeyError('cannot set \"%s\" in \"%s\" (%s)' % (restOfKey,\n",
      "                    myKey, repr(target)))\n",
      "            target[restOfKey] = value\n",
      "        else:\n",
      "            if isinstance(value, dict) and not isinstance(value, dotdictify):\n",
      "                value = dotdictify(value)\n",
      "            dict.__setitem__(self, key, value)\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        if '.' not in key:\n",
      "            return dict.__getitem__(self, key)\n",
      "        myKey, restOfKey = key.split('.', 1)\n",
      "        target = dict.__getitem__(self, myKey)\n",
      "        if isinstance(target, dict):\n",
      "            target = dotdictify(target)\n",
      "        if not isinstance(target, dotdictify):\n",
      "            raise KeyError('cannot get \"%s\" in \"%s\" (%s)' % (restOfKey,\n",
      "                myKey, repr(target)))\n",
      "        return target[restOfKey]\n",
      "\n",
      "    def __contains__(self, key):\n",
      "        if '.' not in key:\n",
      "            return dict.__contains__(self, key)\n",
      "        myKey, restOfKey = key.split('.', 1)\n",
      "        target = dict.__getitem__(self, myKey)\n",
      "        if not isinstance(target, dotdictify):\n",
      "            return False\n",
      "        return restOfKey in target\n",
      "\n",
      "    def get(self, key, default=None):\n",
      "        try:\n",
      "            return self.__getitem__(key)\n",
      "        except KeyError:\n",
      "            return default\n",
      "\n",
      "    def setdefault(self, key, default):\n",
      "        if key not in self:\n",
      "            self[key] = default\n",
      "        return self[key]\n",
      "\u0003\n"
     ]
    }
   ],
   "source": [
    "train_sample = random.sample(training_data, 1)[0]\n",
    "print(\"\".join(train_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Sliding Window Algorithm\n",
    "\n",
    "Extending from the previously created sliding window algorithm:\n",
    "\n",
    "* Given some iterable, we want a generator that yields X, Y pairs for evaluation.\n",
    "* We want a sequence of a given context length as X, and the next character as Y.\n",
    "\n",
    "We want our sliding window algorithm to yield these pairs in batches, for more efficient computation. Iterable is no longer a single file, but an iterator over multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sliding_window_generator(iterable_files, batch_size, max_window_size=None, gen_forever=True):\n",
    "    \"\"\"Sliding window generator for batching files of ASCII characters\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        data_pairs = []\n",
    "\n",
    "        for iterable in iterable_files:\n",
    "            if max_window_size is None:\n",
    "                window_size = len(iterable)\n",
    "            else:\n",
    "                window_size = max_window_size\n",
    "\n",
    "            # edge case inbetween files\n",
    "            existing_window_sizes = [len(x) for x, _ in data_pairs]\n",
    "            if existing_window_sizes and not all(size == window_size for size in existing_window_sizes):\n",
    "                window_size = min(existing_window_sizes)\n",
    "\n",
    "            for window_idx in range(1, len(iterable)):\n",
    "                x = iterable[:window_idx]\n",
    "                if len(x) > window_size:\n",
    "                    x = x[-window_size:]\n",
    "                elif len(x) < window_size:\n",
    "                    x = (PAD,) * (window_size - len(x)) + tuple(x)\n",
    "                y = iterable[window_idx]\n",
    "                \n",
    "                # development, slow check for correctness\n",
    "                # assert \"\".join(\n",
    "                #     (char for char in (tuple(x) + (y,)) if char != PAD)\n",
    "                # ) in \"\".join(iterable)\n",
    "\n",
    "                data_pairs.append((x, y))\n",
    "\n",
    "            random.shuffle(data_pairs)\n",
    "            while len(data_pairs) >= batch_size:\n",
    "                to_yield = data_pairs[:batch_size]\n",
    "                data_pairs = data_pairs[batch_size:]\n",
    "                \n",
    "                # order by sequence lengths\n",
    "                sorted_to_yield = sorted(to_yield, key=lambda kv: kv[0].count(PAD))\n",
    "                yield sorted_to_yield, torch.tensor([len(x) - x.count(PAD) for x, _ in sorted_to_yield])\n",
    "\n",
    "        # To ensure all batches have the same size and no data is left unused, re-sample last file\n",
    "        if data_pairs:\n",
    "            assert len(data_pairs) < batch_size\n",
    "            cleanup_data_pairs = []\n",
    "            for window_idx in range(1, len(iterable)):\n",
    "                x = iterable[:window_idx]\n",
    "                if len(x) > window_size:\n",
    "                    x = x[-window_size:]\n",
    "                elif len(x) < window_size:\n",
    "                    x = (PAD,) * (window_size - len(x)) + tuple(x)\n",
    "                y = iterable[window_idx]\n",
    "                cleanup_data_pairs.append((x, y))\n",
    "            cleanup_data_pairs = random.sample(cleanup_data_pairs, batch_size - len(data_pairs))\n",
    "            data_pairs.extend(cleanup_data_pairs)\n",
    "            # development, check for correctness\n",
    "            assert len(data_pairs) == batch_size\n",
    "            data_pairs.sort(key=lambda kv: kv[0].count(PAD))\n",
    "            yield data_pairs, torch.tensor([len(x) - x.count(PAD) for x, _ in data_pairs])\n",
    "                \n",
    "        if not gen_forever:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 3 generator outputs, given a batch size of 2 and a maximum window size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "([' ', '=', '=', ' ', 'y', '.', 's', 'h', 'a', 'p'], 'e')\n",
      "(['s', 'e', 't', '_', 'f', 'i', 'x', 'e', 'd', '_'], 't')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Batch 1\n",
      "(['e', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' '], 'a')\n",
      "(['r', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'i'], 'n')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Batch 2\n",
      "([']', ':', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ' ')\n",
      "([' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'C', 'r'], 'e')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Padding Example\n",
      "(('\\x00', '\\x00', '\\x00', '\\x00', '\\x00', '\\x00', '\\x00', '\\x00', '\\x02', '\"'), '\"')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_window_size = 10\n",
    "\n",
    "gen = batch_sliding_window_generator(training_data, batch_size, max_window_size)\n",
    "for i in range(3):\n",
    "    batch, _ = next(gen)\n",
    "    print(\"Batch {}\".format(i))\n",
    "    print(*batch, sep=\"\\n\")\n",
    "    print(\"~\"*25)\n",
    "\n",
    "print(\"Padding Example\")\n",
    "searching = True\n",
    "while searching:\n",
    "    batch, _ = next(gen)\n",
    "    for x, y in batch:\n",
    "        if PAD in x:\n",
    "            searching = False\n",
    "            break\n",
    "print((x, y,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how changing the `batch_size` and `max_window_size` parameters of the generator influence the values we use to train our neural network.\n",
    "\n",
    "Setting `batch_size=1` and `max_window_size=None` is most computationally expensive option, which has equivalent functionality to the batching algorithm defined in `char_rnn_one_file_code_gen.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the dynamic batch window size logic works for arbitrary sequence lengths\n",
    "\n",
    "# This takes some time to run. Set this to 0 if you understand the batching generator functionality\n",
    "check_batch_size = 0\n",
    "# check_batch_size = 1\n",
    "# check_batch_size = 100\n",
    "\n",
    "# If set to None, window sizes will default to the number of characters in the file\n",
    "# check_window_size = 10\n",
    "check_window_size = None\n",
    "\n",
    "if check_batch_size:\n",
    "    print(\"Checking batch_size={}, max_window_size={}:\".format(check_batch_size, check_window_size))\n",
    "    gen = batch_sliding_window_generator(\n",
    "        training_data, check_batch_size, max_window_size=check_window_size, gen_forever=False)\n",
    "\n",
    "    batch_windows = {}\n",
    "    for batch, _ in gen:\n",
    "        x, _ = batch[0]\n",
    "        batch_window_size = len(x)\n",
    "        assert all(batch_window_size == len(x) for x, _ in batch)\n",
    "        batch_windows[batch_window_size] = batch_windows.get(batch_window_size, 0) + 1\n",
    "    print(\" • {} number of batches\".format(sum(batch_windows.values())))\n",
    "    for window_size, num_batches in sorted(batch_windows.items(), key=lambda kv: kv[1]):\n",
    "        print(\"   • {} batches have window size {}\".format(num_batches, window_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional tensor containing the original lengths of all the padded sequences is also outputted for each batch. It is sorted by length in decreasing order, as per the [pad_packed_sequence](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Sequence 0 has length 5757 (1552 pad chars)\n",
      "X_Sequence 1 has length 5595 (1714 pad chars)\n",
      "X_Sequence 2 has length 5553 (1756 pad chars)\n",
      "X_Sequence 3 has length 4713 (2596 pad chars)\n",
      "X_Sequence 4 has length 3359 (3950 pad chars)\n",
      "X_Sequence 5 has length 2545 (4764 pad chars)\n",
      "X_Sequence 6 has length 2101 (5208 pad chars)\n",
      "X_Sequence 7 has length 577 (6732 pad chars)\n",
      "X_Sequence 8 has length 256 (7053 pad chars)\n",
      "X_Sequence 9 has length 214 (7095 pad chars)\n"
     ]
    }
   ],
   "source": [
    "check_batch_size = 10\n",
    "check_window_size = None\n",
    "gen = batch_sliding_window_generator(\n",
    "    training_data, check_batch_size, max_window_size=check_window_size, gen_forever=False)\n",
    "\n",
    "batch, x_seqs_len = next(gen)\n",
    "x_seqs, _ = zip(*batch)\n",
    "\n",
    "for idx, x_seq in enumerate(x_seqs):\n",
    "    x_seq_len = x_seqs_len[idx].item()\n",
    "    print(\"X_Sequence {} has length {} ({} pad chars)\".format(idx, x_seq_len, len(x_seq) - x_seq_len))\n",
    "\n",
    "num_pads = len(x_seqs[0]) - x_seqs_len[0].item()\n",
    "# print(x_seqs[0][num_pads:][:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Characters to Tensors\n",
    "\n",
    "Our batch of inputs and outputs must be converted into Tensors for training.\n",
    "\n",
    "We follow the default matrix convention used by the PyTorch comunity.\n",
    "\n",
    "> Tensor’s data will be of size `T x B x *`, where `T` is the length of the longest sequence and `B` is the batch size. \n",
    "\n",
    "To make a batch training example, we join a bunch our sequences of one-hot characters into a matrix of size `(window_size, batch_size, num_chars)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_xy_to_tensor_xy(batch, num_chars=len(CHARACTERS)):\n",
    "    batch_size = len(batch)\n",
    "    window_size = max([len(x) for x, _ in batch])\n",
    "    assert all(len(x) == window_size for x, _ in batch)\n",
    "    \n",
    "    x_tensor = torch.zeros(window_size, batch_size, num_chars)\n",
    "    y_tensor = torch.zeros(1, batch_size, num_chars)\n",
    "\n",
    "    for batch_elm_idx, xy_pair in enumerate(batch):\n",
    "        x_char_seq, y_char = xy_pair\n",
    "        for seq_idx, x_char in enumerate(x_char_seq):\n",
    "            x_tensor[seq_idx][batch_elm_idx][CHAR2INT[x_char]] = 1\n",
    "        y_tensor[0][batch_elm_idx][CHAR2INT[y_char]] = 1\n",
    "    return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor inputs and outputs for the generator function we defined earlier are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 2, max_window_size: 10\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[')', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "['t', 'e', ':', ':', '\\n', ' ', ' ', ' ', ' ', ' ']\n",
      "torch.Size([10, 2, 100])\n",
      "'A'\n",
      "' '\n",
      "torch.Size([1, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "gen = batch_sliding_window_generator(training_data, batch_size, max_window_size)\n",
    "\n",
    "batch, _ = next(gen)\n",
    "x = [item[0] for item in batch]\n",
    "y = [repr(item[1]) for item in batch]\n",
    "x_tensor, y_tensor = batch_xy_to_tensor_xy(batch)\n",
    "\n",
    "print(\"batch_size: {}, max_window_size: {}\".format(batch_size, max_window_size))\n",
    "print(\"~\" * 25)\n",
    "print(*x, sep=\"\\n\")\n",
    "print(x_tensor.size())\n",
    "print(*y, sep=\"\\n\")\n",
    "print(y_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Network\n",
    "\n",
    "We are going to create our recurrent neural network. Rather than using our own pure recurrent neural network defined in the last notebook, we will be using the [recurrent layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers) provided by PyTorch.\n",
    "* **RNN**: [Recurrent Neural Network](https://pytorch.org/docs/stable/nn.html#rnn)\n",
    "* **GRU**: [Gated Recurrent Unit](https://pytorch.org/docs/stable/nn.html#gru)\n",
    "* **LSTM**: [Long Short-Term Memory](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "\n",
    "There are some subtleties in the forward pass function, as we want to mask away the `PAD` characters so they do not influence our model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size,\n",
    "                 hidden_size=128, recurrent_layer_type=\"RNN\", recurrent_layers=1, recurrent_dropout=0):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recurrent_num_layers = recurrent_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        rn_kwargs = {\n",
    "            \"input_size\": input_size,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": recurrent_layers,\n",
    "            \"dropout\": recurrent_dropout,\n",
    "        }\n",
    "        \n",
    "        if recurrent_layer_type == \"RNN\":\n",
    "            self.input_to_rn = nn.RNN(**rn_kwargs)\n",
    "        elif recurrent_layer_type == \"LSTM\":\n",
    "            self.input_to_rn = nn.LSTM(**rn_kwargs)\n",
    "        elif recurrent_layer_type == \"GRU\":\n",
    "            self.input_to_rn = nn.GRU(**rn_kwargs)\n",
    "        else:\n",
    "            raise \"Invalid recurrent layer type: {}\".format(recurrent_layer_type)\n",
    "\n",
    "        self.rn_to_output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_seqs, input_seqs_len, hidden):\n",
    "        seq_len, batch_size, _ = input_seqs.size()\n",
    "        \n",
    "        # pack > recurrent network > unpack\n",
    "        packed_input_seqs = nn.utils.rnn.pack_padded_sequence(input_seqs, input_seqs_len)\n",
    "        rn_outs, hidden = self.input_to_rn(packed_input_seqs, hidden)\n",
    "        unpacked_rn_outs, _ = nn.utils.rnn.pad_packed_sequence(rn_outs, padding_value=CHAR2INT[PAD])\n",
    "\n",
    "        # send rn output through dense linear layer with softmax activation\n",
    "        pre_activated_output = self.rn_to_output(unpacked_rn_outs)\n",
    "        output = self.softmax(pre_activated_output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.recurrent_num_layers, self.batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize this network with values appropriate for the character prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(CHARACTERS)\n",
    "batch_size = 10\n",
    "\n",
    "char_rnn = CharRNN(n_chars, n_chars, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before each batch, we zero out the RNN hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10, 100])\n",
      "torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "max_window_size = 100\n",
    "gen = batch_sliding_window_generator(training_data, batch_size, max_window_size, gen_forever=False)\n",
    "batch, x_seqs_len = next(gen)\n",
    "\n",
    "x_tensor, y_tensor = batch_xy_to_tensor_xy(batch)\n",
    "\n",
    "\n",
    "hidden = char_rnn.init_hidden()\n",
    "output, next_hidden = char_rnn(x_tensor, x_seqs_len, hidden)\n",
    "print(output.size())\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
