{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Sequence Prediction Using Character Level Recurrent Networks\n",
    "\n",
    "We extend the approach outlined in `char_rnn_one_file_code_gen.ipynb` to improve efficiency and to accomodate other recurrent cells. Notably:\n",
    "\n",
    "* Training will use multiple files, instead of a single file\n",
    "* Validation sets will be introduced to avoid overfitting on our training data\n",
    "* We will have our model utilize mini-batches to speed up training\n",
    "\n",
    "Our outlined task is still the same.\n",
    "\n",
    "**Given a sequence of characters, predict the next likely character in the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from json import load\n",
    "from random import shuffle\n",
    "from math import floor\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We will be training on a set of 900 preprocessed Python files (and validating on a set of 100 other files) arbitrarily sampled from [GitHub BigQuery Python Extracts](https://bigquery.cloud.google.com/table/fh-bigquery:github_extracts.contents_py_201802snap?pli=1).\n",
    "\n",
    "We limit the characters that our neural network can produce to a subset of standard ASCII.\n",
    "* `ORD 2*, 3*, 9, 10, 32-126`\n",
    "  * `ORD 2` for start of text (special, never predicted)\n",
    "  * `ORD 3` for end of text (special, prediction ends)\n",
    "  * `ORD 9` horizontal tab \"\\t\"\n",
    "  * `ORD 10` NL line feed, new line \"\\n\"\n",
    "  * `ORD 32-126` Space, Punctuation, Digits, English Letters\n",
    "\n",
    "NOTE: The full dataset contains files written using non standard characters. For the models in this notebook, we ensure that all Python files within our dataset are composed only of ASCII characters that we accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: '\\x02', 3: '\\x03', 9: '\\t', 10: '\\n', 32: ' ', \n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "Num Training Files: 180\n",
      "Num Validation Files: 20\n"
     ]
    }
   ],
   "source": [
    "VALID_UNICODE_IDS = (2, 3, 9, 10) + tuple(range(32, 127))\n",
    "for uid in VALID_UNICODE_IDS:\n",
    "    if uid <= 32:\n",
    "        print(\"{}: {}\".format(uid, repr(chr(uid))), end=\", \")\n",
    "        continue\n",
    "    elif uid == 33:\n",
    "        print()\n",
    "    print(chr(uid), end=\"\")\n",
    "print()\n",
    "\n",
    "with open(\"./data/train.json\", \"r\") as f:\n",
    "    train_data = load(f)\n",
    "with open(\"./data/validate.json\", \"r\") as f:\n",
    "    validation_data = load(f)\n",
    "\n",
    "print(\"Num Training Files: {}\".format(len(train_data)))\n",
    "print(\"Num Validation Files: {}\".format(len(validation_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
