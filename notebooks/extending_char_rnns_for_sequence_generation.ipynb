{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Character Level Recurrent Networks for Sequence Generation\n",
    "\n",
    "We extend the approach outlined in `char_rnn_one_file_code_gen.ipynb` to improve efficiency and to accomodate other recurrent layer types. Notably, we introduce:\n",
    "\n",
    "* Training will use multiple files instead of a single file\n",
    "* Validation sets will be introduced to avoid overfitting on our training data\n",
    "* Mini-batches for training speedup\n",
    "\n",
    "Our outlined task is still the same.\n",
    "\n",
    "**Given a sequence of characters, predict the next likely character in the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from json import load\n",
    "from math import floor\n",
    "from time import time\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We will be training on a set of 180 preprocessed Python files (and validating on a set of 20 other files) arbitrarily sampled from [GitHub BigQuery Python Extracts](https://bigquery.cloud.google.com/table/fh-bigquery:github_extracts.contents_py_201802snap?pli=1).\n",
    "\n",
    "We limit the characters that our neural network can produce to a subset of standard ASCII.\n",
    "* `ORD 2*, 3*, 9, 10, 32-126`\n",
    "  * `ORD 0` for padding (special, used in batch_size > 1 with variable length sequences)\n",
    "  * `ORD 2` for start of text (special, predicted sequence start)\n",
    "  * `ORD 3` for end of text (special, sequence prediction ends)\n",
    "  * `ORD 9` horizontal tab \"\\t\"\n",
    "  * `ORD 10` NL line feed, new line \"\\n\"\n",
    "  * `ORD 32-126` Space, Punctuation, Digits, English Letters\n",
    "\n",
    "NOTE: The full dataset contains files written using non standard characters. For the models in this notebook, we ensure that all Python files within our dataset are composed only of ASCII characters that we accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\\x00'\n",
      "2: '\\x02'\n",
      "3: '\\x03'\n",
      "9: '\\t'\n",
      "10: '\\n'\n",
      "32: ' '\n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Num Training Files: 180\n",
      "Num Validation Files: 20\n",
      "SANITY Training Files: 1\n",
      "SANITY Validation Files: 1\n"
     ]
    }
   ],
   "source": [
    "# Possible Characters for neural network\n",
    "VALID_UNICODE_IDS = (0, 2, 3, 9, 10) + tuple(range(32, 127))\n",
    "for uid in VALID_UNICODE_IDS:\n",
    "    if uid <= 32:\n",
    "        print(\"{}: {}\".format(uid, repr(chr(uid))))\n",
    "        continue\n",
    "    print(chr(uid), end=\"\")\n",
    "print()\n",
    "\n",
    "# Special Characters\n",
    "PAD = chr(0)\n",
    "FILE_START = chr(2)\n",
    "FILE_END = chr(3)\n",
    "\n",
    "CHARACTERS = set(chr(id) for id in VALID_UNICODE_IDS)\n",
    "INT2CHAR = dict(enumerate(CHARACTERS))\n",
    "CHAR2INT = {char: idx for idx, char in INT2CHAR.items()}\n",
    "\n",
    "with open(\"./data/train.json\", \"r\") as f:\n",
    "    training_data = load(f)\n",
    "with open(\"./data/validate.json\", \"r\") as f:\n",
    "    validation_data = load(f)\n",
    "\n",
    "print(\"~\" * 25)\n",
    "print(\"Num Training Files: {}\".format(len(training_data)))\n",
    "print(\"Num Validation Files: {}\".format(len(validation_data)))\n",
    "\n",
    "# SANITY_CHECK, MEMORIZE ONE FILE\n",
    "with open(\"./data/test.py\", \"r\") as f:\n",
    "    text = f.read()\n",
    "training_data = ((FILE_START, ) + tuple(text) + (FILE_END,),)\n",
    "validation_data = ((FILE_START, ) + tuple(text) + (FILE_END,),)\n",
    "print(\"SANITY Training Files: {}\".format(len(training_data)))\n",
    "print(\"SANITY Validation Files: {}\".format(len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly take a look at what is contained within our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002\"\"\"Predict Test\"\"\"\n",
      "import sys\n",
      "from os import getcwd\n",
      "\n",
      "def main():\n",
      "    sys.stdout.write(getcwd())\n",
      "    for i in range(0, 10):\n",
      "        print(\"{} : Boop\".format(i), i)\n",
      "    return False\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\u0003\n"
     ]
    }
   ],
   "source": [
    "train_sample = random.choice(training_data)\n",
    "print(\"\".join(train_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize CUDA & GPU\n",
    "\n",
    "Training neural networks can be slow. Utilize GPUs if they are available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      " • Number of CUDA devices: 1\n",
      " • Current Device Name: TITAN Xp\n",
      " • Device CUDA Capability: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "HAS_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    HAS_CUDA = True\n",
    "    print(\"CUDA is available\")\n",
    "    print(\" • Number of CUDA devices: {}\".format(torch.cuda.device_count()))\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\" • Current Device Name: {}\".format(torch.cuda.get_device_name(device)))\n",
    "    print(\" • Device CUDA Capability: {}\".format(torch.cuda.get_device_capability(device)))\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# force GPU off.\n",
    "HAS_CUDA = False\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Sliding Window Algorithm\n",
    "\n",
    "We extend from the previously created sliding window algorithm, but rather than predicting the next character, we output the next sequence Y as one character shifted over:\n",
    "\n",
    "* Given some iterable, we want a generator that yields X, Y pairs for evaluation.\n",
    "* We want a sequence of a given context length as X, and the sequence one character shifted over as Y.\n",
    "\n",
    "We want our sliding window algorithm to yield these pairs in batches, for more efficient computation. Iterable is no longer a single file, but an iterator over multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sliding_window_generator(files_iterable, batch_size, max_window_size=None, gen_forever=True):\n",
    "    \"\"\"Sliding window generator for batching files of ASCII characters\"\"\"\n",
    "    while True:\n",
    "        for iterable in files_iterable:\n",
    "            if max_window_size is None:\n",
    "                window_size = len(iterable) - 1\n",
    "            else:\n",
    "                window_size = min(max_window_size, len(iterable) - 1)\n",
    "\n",
    "            inp_batch = []\n",
    "            target_batch = []\n",
    "            # each batch should consist of a single file\n",
    "            for batch_item in range(batch_size):\n",
    "                start_index = random.randint(0, len(iterable) - window_size)\n",
    "                end_index = start_index + window_size + 1\n",
    "                chunk = iterable[start_index:end_index]\n",
    "                inp_batch.append(chunk[:-1])\n",
    "                target_batch.append(chunk[1:])\n",
    "            \n",
    "            yield inp_batch, target_batch\n",
    "        if not gen_forever:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first few generator outputs.\n",
    "Note the provided batch sizes and window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Input:\n",
      "('r', 'i', 't', 'e', '(', 'g', 'e', 't', 'c', 'w', 'd', '(', ')', ')', '\\n')\n",
      "('o', 'r', 't', ' ', 's', 'y', 's', '\\n', 'f', 'r', 'o', 'm', ' ', 'o', 's')\n",
      "Target: \n",
      "('i', 't', 'e', '(', 'g', 'e', 't', 'c', 'w', 'd', '(', ')', ')', '\\n', ' ')\n",
      "('r', 't', ' ', 's', 'y', 's', '\\n', 'f', 'r', 'o', 'm', ' ', 'o', 's', ' ')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Batch 1\n",
      "Input:\n",
      "('o', 'r', 'm', 'a', 't', '(', 'i', ')', ',', ' ', 'i', ')', '\\n', ' ', ' ')\n",
      "('r', 'a', 'n', 'g', 'e', '(', '0', ',', ' ', '1', '0', ')', ':', '\\n', ' ')\n",
      "Target: \n",
      "('r', 'm', 'a', 't', '(', 'i', ')', ',', ' ', 'i', ')', '\\n', ' ', ' ', ' ')\n",
      "('a', 'n', 'g', 'e', '(', '0', ',', ' ', '1', '0', ')', ':', '\\n', ' ', ' ')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Batch 2\n",
      "Input:\n",
      "('\\n', 'f', 'r', 'o', 'm', ' ', 'o', 's', ' ', 'i', 'm', 'p', 'o', 'r', 't')\n",
      "('_', '\"', ':', '\\n', ' ', ' ', ' ', ' ', 'm', 'a', 'i', 'n', '(', ')', '\\n')\n",
      "Target: \n",
      "('f', 'r', 'o', 'm', ' ', 'o', 's', ' ', 'i', 'm', 'p', 'o', 'r', 't', ' ')\n",
      "('\"', ':', '\\n', ' ', ' ', ' ', ' ', 'm', 'a', 'i', 'n', '(', ')', '\\n', '\\x03')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_window_size = 15\n",
    "\n",
    "gen = batch_sliding_window_generator(training_data, batch_size, max_window_size)\n",
    "for i in range(3):\n",
    "    inp, target = next(gen)\n",
    "    print(\"Batch {}\".format(i))\n",
    "    print(\"Input:\")\n",
    "    print(*inp, sep=\"\\n\")\n",
    "    print(\"Target: \")\n",
    "    print(*target, sep=\"\\n\")\n",
    "    print(\"~\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Characters to Tensors\n",
    "\n",
    "Our batch of inputs and outputs must be converted into Tensors for training.\n",
    "\n",
    "We follow the default matrix convention used by the PyTorch comunity.\n",
    "\n",
    "> Tensor’s data will be of size `T x B x *`, where `T` is the length of the longest sequence and `B` is the batch size. \n",
    "\n",
    "To make a batch training example, we join a bunch our sequences of one-hot characters into a matrix of size `(window_size, batch_size, num_chars)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_tensor(char, num_chars=len(CHARACTERS)):\n",
    "    tensor = torch.zeros(1, 1, num_chars, device=device)\n",
    "    tensor[0][0][CHAR2INT[char]] = 1\n",
    "    return tensor\n",
    "\n",
    "def charseq_to_tensor(charseq, num_chars=len(CHARACTERS)):\n",
    "    window_size = len(charseq)\n",
    "    tensor = torch.zeros(window_size, 1, num_chars, device=device)\n",
    "    for seq_idx, seq_item in enumerate(charseq):\n",
    "        tensor[seq_idx][0][CHAR2INT[seq_item]] = 1\n",
    "    return tensor\n",
    "\n",
    "def charseqs_to_tensor(charseqs, num_chars=len(CHARACTERS)):\n",
    "    batch_size = len(charseqs)\n",
    "    window_size = max([len(v) for v in charseqs])\n",
    "    assert all(len(v) == window_size for v in charseqs)\n",
    "    \n",
    "    tensor = torch.zeros(window_size, batch_size, num_chars, device=device)\n",
    "    \n",
    "    for batch_elm_idx, batch_item in enumerate(charseqs):\n",
    "        for seq_idx, seq_item in enumerate(batch_item):\n",
    "            tensor[seq_idx][batch_elm_idx][CHAR2INT[seq_item]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor inputs and outputs for the generator function we defined earlier are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 2, max_window_size: 15\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "('i', 'n', 't', '(', '\"', '{', '}', ' ', ':', ' ', 'B', 'o', 'o', 'p', '\"')\n",
      "('e', 't', 'u', 'r', 'n', ' ', 'F', 'a', 'l', 's', 'e', '\\n', '\\n', 'i', 'f')\n",
      "torch.Size([15, 2, 100])\n",
      "('n', 't', '(', '\"', '{', '}', ' ', ':', ' ', 'B', 'o', 'o', 'p', '\"', '.')\n",
      "('t', 'u', 'r', 'n', ' ', 'F', 'a', 'l', 's', 'e', '\\n', '\\n', 'i', 'f', ' ')\n",
      "torch.Size([15, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "gen = batch_sliding_window_generator(training_data, batch_size, max_window_size)\n",
    "\n",
    "inp_val, target_val = next(gen)\n",
    "inp_tensor = charseqs_to_tensor(inp_val)\n",
    "target_tensor = charseqs_to_tensor(target_val)\n",
    "\n",
    "print(\"batch_size: {}, max_window_size: {}\".format(batch_size, max_window_size))\n",
    "print(\"~\" * 25)\n",
    "print(*inp_val, sep=\"\\n\")\n",
    "print(inp_tensor.size())\n",
    "#_, idxs = inp_tensor.topk(1)\n",
    "print(*target_val, sep=\"\\n\")\n",
    "print(target_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Network\n",
    "\n",
    "We are going to create our recurrent neural network. Rather than using our own pure recurrent neural network defined in the last notebook, we will be using the recurrent layers provided by PyTorch.\n",
    "\n",
    "* **RNN**: [Recurrent Neural Network](https://pytorch.org/docs/stable/nn.html#rnn)\n",
    "* **GRU**: [Gated Recurrent Unit](https://pytorch.org/docs/stable/nn.html#gru)\n",
    "* **LSTM**: [Long Short-Term Memory](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "\n",
    "Our model is composed of the following:\n",
    "0. Recurrent Layer (RNN/GRU/LSTM)\n",
    "0. Linear Layer\n",
    "0. Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size,\n",
    "                 hidden_size=128, recurrent_type=\"LSTM\", recurrent_layers=1, recurrent_dropout=0):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.recurrent_type = recurrent_type.upper()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recurrent_layers = recurrent_layers\n",
    "        self.recurrent_dropoout = recurrent_dropout\n",
    "        \n",
    "        rn_kwargs = {\n",
    "            \"input_size\": input_size,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": recurrent_layers,\n",
    "            \"dropout\": recurrent_dropout,\n",
    "        }\n",
    "        \n",
    "        if self.recurrent_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(**rn_kwargs)\n",
    "        elif self.recurrent_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(**rn_kwargs)\n",
    "        elif self.recurrent_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(**rn_kwargs)\n",
    "        else:\n",
    "            raise \"Invalid recurrent layer type: {}\".format(recurrent_type)\n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, inp_val, hidden):\n",
    "        for_decoder, hidden = self.rnn(inp_val, hidden)\n",
    "        for_softmax = self.decoder(for_decoder)\n",
    "        output = self.softmax(for_softmax)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.recurrent_type == \"LSTM\":\n",
    "            return (\n",
    "                torch.zeros(self.recurrent_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.recurrent_layers, batch_size, self.hidden_size, device=device)\n",
    "            )\n",
    "        return torch.zeros(self.recurrent_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize this network with values appropriate for the character prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(CHARACTERS)\n",
    "batch_size = 4\n",
    "\n",
    "char_rnn = CharRNN(n_chars, n_chars)\n",
    "if HAS_CUDA:\n",
    "    char_rnn.cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before each batch, the RNN's hidden state should be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([20, 4, 100])\n",
      "Output: torch.Size([20, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "max_window_size = 20\n",
    "gen = batch_sliding_window_generator(\n",
    "    training_data, batch_size, max_window_size, gen_forever=False)\n",
    "inp_charseq, target_charseq = next(gen)\n",
    "inp_val = charseqs_to_tensor(inp_charseq)\n",
    "\n",
    "hidden = char_rnn.init_hidden(batch_size)\n",
    "\n",
    "output, hidden = char_rnn(inp_val, hidden)\n",
    "print(\"Input:\", inp_val.size())\n",
    "print(\"Output:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Train our Model\n",
    "\n",
    "We will create a helper function to convert the network predicted output back to a human readable character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: ]]]]]]]]]]]]]]]]]]]]\n",
      "Batch 1: ]]]]]]]]]]]]]]]]]]]]\n",
      "Batch 2: ]]]]]]]]]]]]]]]]]]]]\n",
      "Batch 3: ]]]]]]]]]]]]]]]]]]]]\n"
     ]
    }
   ],
   "source": [
    "def readable_from_output(output):\n",
    "    \"\"\"Convert the network output predictions back to readable values.\"\"\"\n",
    "    _, top_i = output.topk(1)\n",
    "    window_size, batch_size, _ = top_i.size()\n",
    "    batches_chars = []\n",
    "    for batch_ele_idx in range(batch_size):\n",
    "        batch_chars = []\n",
    "        for seq_ele_idx in range(window_size):\n",
    "            char_idx = top_i[seq_ele_idx][batch_ele_idx].item()\n",
    "            char = INT2CHAR[char_idx], char_idx\n",
    "            batch_chars.append(char)\n",
    "        batches_chars.append(batch_chars)\n",
    "    return batches_chars\n",
    "\n",
    "def pretty_print_output(batches_chars):\n",
    "    for seq_idx, seq in enumerate(batches_chars):\n",
    "        print(\"Batch {}:\".format(seq_idx), \"\".join([char for char, _ in seq]))\n",
    "\n",
    "batches_chars = readable_from_output(output)\n",
    "pretty_print_output(batches_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the network some examples, have it make predictions, and then inform the network when the predictions are correct.\n",
    "We have a classification problem, so Negative Log Likelihood loss is appropriate.\n",
    "\n",
    "For classifing input into `C` number of classes, the following loss functions are useful:\n",
    "\n",
    "* **Cross Entropy Loss**: [CrossEntropyLoss docs](https://pytorch.org/docs/stable/nn.html#crossentropyloss)\n",
    "* **Negative Log Likelihood Loss**: [NLLLoss docs](https://pytorch.org/docs/stable/nn.html#nllloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training loop will:\n",
    "0. Create input and target tensors\n",
    "0. Initialize a zerored hidden state\n",
    "0. Read each letter in and keep the hidden state for the next letter\n",
    "0. Compare the final output to the target\n",
    "0. Back-propagate\n",
    "0. Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(target_tensor, input_tensor, optimizer):\n",
    "    window_size, batch_size, _ = input_tensor.size()\n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    \n",
    "    # Set model gradients to zero\n",
    "    char_rnn.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    correct = []\n",
    "    for i in range(window_size):\n",
    "        output, hidden = char_rnn(input_tensor.narrow(0, i, 1), hidden)\n",
    "        _, loss_output_tensor = target_tensor[i].topk(1)\n",
    "        loss += criterion(output.view(1, -1), loss_output_tensor.view(1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # diagnose training issues\n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "    return output, loss.item() / window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We want to also evaluate the effectiveness of our model on a validation set, without updating the model.\n",
    "\n",
    "This is done in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target_tensor, input_tensor):\n",
    "    window_size, batch_size, _ = input_tensor.size()\n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    \n",
    "    # Set model gradients to zero\n",
    "    char_rnn.zero_grad()\n",
    "    loss = 0\n",
    "    match = []\n",
    "    \n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    for i in range(window_size):\n",
    "        output, hidden = char_rnn(input_tensor.narrow(0, i, 1), hidden)\n",
    "        _, target_character = target_tensor[i].topk(1)\n",
    "        _, predict_character = output.topk(1)\n",
    "        \n",
    "        if target_character.view(1).item() == predict_character.view(1).item():\n",
    "            match.append(1)\n",
    "        else:\n",
    "            match.append(0)\n",
    "        \n",
    "        loss += criterion(output.view(1, -1), target_character.view(1))\n",
    "    \n",
    "    # loss.backward()\n",
    "    accuracy = sum(match)/len(match)\n",
    "    return accuracy, loss.item() / window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Model\n",
    "\n",
    "Let's train our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required variables\n",
    "n_chars = len(CHARACTERS)\n",
    "batch_size = 1\n",
    "char_rnn = CharRNN(n_chars, n_chars, recurrent_type=\"LSTM\")\n",
    "if HAS_CUDA:\n",
    "    char_rnn.cuda(device)\n",
    "learning_rate = 0.0005\n",
    "char_rnn_optimizer = torch.optim.Adam(char_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Helper function\n",
    "def time_since(since):\n",
    "    delta = time() - since\n",
    "    sec = int(delta)\n",
    "    mins = floor(sec / 60)\n",
    "    sec -= mins * 60\n",
    "    return \"{}m {:0>2d}s\".format(mins, sec)\n",
    "\n",
    "# Keep track of all the trainng that is done\n",
    "train_epoch_losses = []\n",
    "eval_epoch_losses = []\n",
    "batch_iter_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable which can change in-between trainng\n",
    "n_epochs = 1500\n",
    "print_every_train = 443\n",
    "print_every_epoch = 100\n",
    "max_window_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 0.0%: iter     0 (0m 00s) | Train Loss: 4.59795\n",
      "> Epoch 100 6.6%: (0m 17s) | Train Loss: 3.14074, Mean Eval Loss: 3.14207, Mean Acc: 0.19\n",
      "> Epoch 200 13.3%: (0m 34s) | Train Loss: 2.87004, Mean Eval Loss: 2.86263, Mean Acc: 0.20\n",
      "> Epoch 300 19.9%: (0m 51s) | Train Loss: 2.74334, Mean Eval Loss: 2.72821, Mean Acc: 0.23\n",
      "> Epoch 400 26.6%: (1m 08s) | Train Loss: 2.30188, Mean Eval Loss: 2.29751, Mean Acc: 0.28\n",
      "Epoch 443 29.5%: iter   443 (1m 15s) | Train Loss: 2.12387\n",
      "> Epoch 500 33.3%: (1m 25s) | Train Loss: 1.89270, Mean Eval Loss: 1.89095, Mean Acc: 0.40\n",
      "> Epoch 600 39.9%: (1m 42s) | Train Loss: 1.47570, Mean Eval Loss: 1.47778, Mean Acc: 0.57\n",
      "> Epoch 700 46.6%: (1m 59s) | Train Loss: 1.11539, Mean Eval Loss: 1.12013, Mean Acc: 0.74\n",
      "> Epoch 800 53.3%: (2m 16s) | Train Loss: 0.83196, Mean Eval Loss: 0.83565, Mean Acc: 0.89\n",
      "Epoch 886 59.1%: iter   886 (2m 31s) | Train Loss: 1.57955\n",
      "> Epoch 900 59.9%: (2m 33s) | Train Loss: 1.49931, Mean Eval Loss: 1.48600, Mean Acc: 0.63\n",
      "> Epoch 1000 66.6%: (2m 50s) | Train Loss: 1.09441, Mean Eval Loss: 1.07425, Mean Acc: 0.76\n",
      "> Epoch 1100 73.3%: (3m 07s) | Train Loss: 0.81685, Mean Eval Loss: 0.81456, Mean Acc: 0.86\n",
      "> Epoch 1200 79.9%: (3m 24s) | Train Loss: 0.63049, Mean Eval Loss: 0.63744, Mean Acc: 0.95\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    # This generator will terminate when it finishes the file.\n",
    "    gen = batch_sliding_window_generator(\n",
    "        training_data, batch_size, max_window_size, gen_forever=False)\n",
    "    for input_char_seq, target_char_seq in gen:\n",
    "        input_tensor = charseqs_to_tensor(input_char_seq)\n",
    "        target_tensor = charseqs_to_tensor(target_char_seq)\n",
    "        \n",
    "        pred_output, train_loss = train(target_tensor, input_tensor, char_rnn_optimizer)\n",
    "\n",
    "        if batch_iter_idx % print_every_train == 0:            \n",
    "            print(\"Epoch {:1d} {:.1f}%: iter {:5d} ({}) | Train Loss: {:.5f}\".format(\n",
    "                len(train_epoch_losses), epoch_idx/n_epochs * 100, batch_iter_idx,\n",
    "                time_since(start), train_loss))\n",
    "        batch_iter_idx += 1\n",
    "\n",
    "    # Add current loss avg to list of losses after each epoch\n",
    "    train_epoch_losses.append(train_loss)\n",
    "\n",
    "    # Run our model on our validation data set to determine validation loss and accuracy\n",
    "    gen = batch_sliding_window_generator(\n",
    "        validation_data, batch_size, max_window_size, gen_forever=False)\n",
    "    \n",
    "    eval_accuracies = []\n",
    "    eval_losses = []\n",
    "    for input_char_seq, target_char_seq in gen:\n",
    "        input_tensor = charseqs_to_tensor(input_char_seq)\n",
    "        target_tensor = charseqs_to_tensor(target_char_seq)\n",
    "        eval_accuracy, eval_loss = evaluate(target_tensor, input_tensor)\n",
    "        eval_accuracies.append(eval_accuracy)\n",
    "        eval_losses.append(eval_loss)\n",
    "\n",
    "    mean_eval_loss = mean(eval_losses)\n",
    "    mean_eval_acc = mean(eval_accuracies)\n",
    "\n",
    "    eval_epoch_losses.append(mean_eval_loss)\n",
    "    \n",
    "    if len(train_epoch_losses) % print_every_epoch == 0:\n",
    "        print(\"> Epoch {:1d} {:.1f}%: ({}) | Train Loss: {:.5f}, Mean Eval Loss: {:.5f}, Mean Acc: {:.2f}\".format(\n",
    "            len(train_epoch_losses), epoch_idx/n_epochs * 100,\n",
    "            time_since(start), train_loss, mean_eval_loss, mean_eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Training Process\n",
    "Plotting the historical loss from train_epoch_losses show how well the network is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Batching Char-RNN Training Loss over Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Negative Log Likelihood\")\n",
    "plt.plot(train_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Batching Char-RNN Evaluation Loss over Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Negative Log Likelihood\")\n",
    "plt.plot(eval_epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using User Specified Input\n",
    "Let's test this ourselves by supplying our own Python source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, prime_str=FILE_START, print_output=True, max_gen_len=1000, temperature=None):\n",
    "    hidden = rnn.init_hidden(1)\n",
    "    if not prime_str.startswith(FILE_START):\n",
    "        prime_str = FILE_START + prime_str\n",
    "\n",
    "    input_seq = list(prime_str)\n",
    "    # use priming sequence to construct the hidden state\n",
    "    input_tensor = charseq_to_tensor(input_seq)\n",
    "    for i in range(len(input_seq)):\n",
    "        output, hidden = rnn(input_tensor.narrow(0, i, 1), hidden)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"\".join(input_seq))\n",
    "        print(\"~~~~Predict~~~~\")\n",
    "        print(\"\".join(input_seq))\n",
    "\n",
    "    # predict until max_len or FILE_END character is reached\n",
    "    predicted = input_seq[:]\n",
    "    for i in range(max_gen_len):\n",
    "        if temperature is not None:\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            char = INT2CHAR[top_i.item()]\n",
    "        else:\n",
    "            _, pred_char_idx = output.topk(1)\n",
    "            char = INT2CHAR[pred_char_idx.item()]\n",
    "        if print_output:\n",
    "            print(char, end=\"\")\n",
    "        predicted.append(char)\n",
    "        if char == FILE_END:\n",
    "            return predicted\n",
    "        input_tensor = char_to_tensor(char)\n",
    "        output, hidden = rnn(input_tensor, hidden)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"~max_gen_len reached~\")\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate(char_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you use this with your own text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate(char_rnn, \"ale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
