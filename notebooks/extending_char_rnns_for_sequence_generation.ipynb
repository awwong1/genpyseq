{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Character Level Recurrent Networks for Sequence Generation\n",
    "\n",
    "We extend the approach outlined in `char_rnn_one_file_code_gen.ipynb` to improve efficiency and to accomodate other recurrent layer types. Notably, we introduce:\n",
    "\n",
    "* Training will use multiple files instead of a single file\n",
    "* Validation sets will be introduced to avoid overfitting on our training data\n",
    "* Mini-batches for training speedup\n",
    "\n",
    "Our outlined task is still the same.\n",
    "\n",
    "**Given a sequence of characters, predict the next likely character in the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from json import load\n",
    "from math import floor\n",
    "from time import time\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We will be training on a set of 180 preprocessed Python files (and validating on a set of 20 other files) arbitrarily sampled from [GitHub BigQuery Python Extracts](https://bigquery.cloud.google.com/table/fh-bigquery:github_extracts.contents_py_201802snap?pli=1).\n",
    "\n",
    "We limit the characters that our neural network can produce to a subset of standard ASCII.\n",
    "* `ORD 2*, 3*, 9, 10, 32-126`\n",
    "  * `ORD 0` for padding (special, used in batch_size > 1 with variable length sequences)\n",
    "  * `ORD 2` for start of text (special, predicted sequence start)\n",
    "  * `ORD 3` for end of text (special, sequence prediction ends)\n",
    "  * `ORD 9` horizontal tab \"\\t\"\n",
    "  * `ORD 10` NL line feed, new line \"\\n\"\n",
    "  * `ORD 32-126` Space, Punctuation, Digits, English Letters\n",
    "\n",
    "NOTE: The full dataset contains files written using non standard characters. For the models in this notebook, we ensure that all Python files within our dataset are composed only of ASCII characters that we accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\\x00'\n",
      "2: '\\x02'\n",
      "3: '\\x03'\n",
      "9: '\\t'\n",
      "10: '\\n'\n",
      "32: ' '\n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Num Training Files: 180\n",
      "Num Validation Files: 20\n"
     ]
    }
   ],
   "source": [
    "# Possible Characters for neural network\n",
    "VALID_UNICODE_IDS = (0, 2, 3, 9, 10) + tuple(range(32, 127))\n",
    "for uid in VALID_UNICODE_IDS:\n",
    "    if uid <= 32:\n",
    "        print(\"{}: {}\".format(uid, repr(chr(uid))))\n",
    "        continue\n",
    "    print(chr(uid), end=\"\")\n",
    "print()\n",
    "\n",
    "# Special Characters\n",
    "PAD = chr(0)\n",
    "FILE_START = chr(2)\n",
    "FILE_END = chr(3)\n",
    "\n",
    "CHARACTERS = set(chr(id) for id in VALID_UNICODE_IDS)\n",
    "INT2CHAR = dict(enumerate(CHARACTERS))\n",
    "CHAR2INT = {char: idx for idx, char in INT2CHAR.items()}\n",
    "\n",
    "with open(\"./data/train.json\", \"r\") as f:\n",
    "    training_data = load(f)\n",
    "with open(\"./data/validate.json\", \"r\") as f:\n",
    "    validation_data = load(f)\n",
    "\n",
    "print(\"~\" * 25)\n",
    "print(\"Num Training Files: {}\".format(len(training_data)))\n",
    "print(\"Num Validation Files: {}\".format(len(validation_data)))\n",
    "\n",
    "# SANITY_CHECK, MEMORIZE TWO FILES\n",
    "#with open(\"./data/test.py\", \"r\") as f:\n",
    "#    text = f.read()\n",
    "#with open(\"./data/test2.py\", \"r\") as f:\n",
    "#    text2 = f.read()\n",
    "#training_data = (\n",
    "#    (FILE_START, ) + tuple(text) + (FILE_END,),\n",
    "#    (FILE_START, ) + tuple(text2) + (FILE_END,),\n",
    "#)\n",
    "#validation_data = training_data[:]\n",
    "#print(\"SANITY Training Files: {}\".format(len(training_data)))\n",
    "#print(\"SANITY Validation Files: {}\".format(len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly take a look at what is contained within our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634\n",
      "\u0002from common.numpy_fast import clip\n",
      "\n",
      "\n",
      "def rate_limit(new_value, last_value, dw_step, up_step):\n",
      "    return clip(new_value, last_value + dw_step, last_value + up_step)\n",
      "\n",
      "\n",
      "def learn_angle_offset(lateral_control, v_ego, angle_offset, d_poly, y_des,\n",
      "    steer_override):\n",
      "    min_offset = -1.0\n",
      "    max_offset = 1.0\n",
      "    alpha = 1.0 / 36000.0\n",
      "    min_learn_speed = 1.0\n",
      "    alpha_v = alpha * max(v_ego - min_learn_speed, 0.0) / (1.0 + 0.5 * abs(\n",
      "        y_des))\n",
      "    if lateral_control and not steer_override:\n",
      "        angle_offset += d_poly[3] * alpha_v\n",
      "        angle_offset = clip(angle_offset, min_offset, max_offset)\n",
      "    return angle_offset\n",
      "\u0003\n"
     ]
    }
   ],
   "source": [
    "train_sample = random.choice(training_data)\n",
    "print(len(train_sample))\n",
    "print(\"\".join(train_sample))\n",
    "# print([repr(v) for v in train_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize CUDA & GPU\n",
    "\n",
    "Training neural networks can be slow. Utilize GPUs if they are available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      " • Number of CUDA devices: 1\n",
      " • Current Device Name: TITAN Xp\n",
      " • Device CUDA Capability: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "HAS_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    HAS_CUDA = True\n",
    "    print(\"CUDA is available\")\n",
    "    print(\" • Number of CUDA devices: {}\".format(torch.cuda.device_count()))\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\" • Current Device Name: {}\".format(torch.cuda.get_device_name(device)))\n",
    "    print(\" • Device CUDA Capability: {}\".format(torch.cuda.get_device_capability(device)))\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# force GPU off.\n",
    "# HAS_CUDA = False\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Sliding Window Algorithm\n",
    "\n",
    "We extend from the previously created sliding window algorithm, but rather than predicting the next character, we output the next sequence Y as one character shifted over:\n",
    "\n",
    "* Given some iterable, we want a generator that yields X, Y pairs for evaluation.\n",
    "* We want a sequence of a given context length as X, and the sequence one character shifted over as Y.\n",
    "\n",
    "We want our sliding window algorithm to yield these pairs in batches, for more efficient computation. Iterable is no longer a single file, but an iterator over multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sliding_window_generator(files_iterable, max_batch_size, max_window_size=None, gen_forever=True):\n",
    "    \"\"\"Sliding window generator for batching files of ASCII characters\"\"\"\n",
    "    inp_batch = []\n",
    "    target_batch = []\n",
    "    while True:\n",
    "        for iterable in files_iterable:\n",
    "            inp_batch = []\n",
    "            target_batch = []\n",
    "\n",
    "            if max_window_size is None:\n",
    "                window_size = len(iterable)\n",
    "            else:\n",
    "                window_size = min(max_window_size, len(iterable) - 1)\n",
    "\n",
    "            chunk_windows = list(range(0, len(iterable) - window_size + 1))\n",
    "            random.shuffle(chunk_windows)\n",
    "            while chunk_windows:\n",
    "                start_index = chunk_windows.pop()\n",
    "                end_index = start_index + window_size + 1\n",
    "                chunk = iterable[start_index:end_index]\n",
    "                if chunk[:-1] and chunk[1:]:\n",
    "                    if inp_batch and len(inp_batch[0]) != len(chunk[:-1]):\n",
    "                        # diagnose training issues\n",
    "                        import pdb; pdb.set_trace()\n",
    "                        continue\n",
    "                    inp_batch.append(chunk[:-1])\n",
    "                    target_batch.append(chunk[1:])\n",
    "                \n",
    "                if len(inp_batch) >= max_batch_size:\n",
    "                    yield inp_batch, target_batch\n",
    "                    inp_batch = []\n",
    "                    target_batch = []\n",
    "            if inp_batch:\n",
    "                yield inp_batch, target_batch\n",
    "        if not gen_forever:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first few generator outputs.\n",
    "Note the provided batch sizes and window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Input:\n",
      "[' ', 'd', 'e', 'f', ' ', '_', '_', 'i', 'n', 'i', 't', '_', '_', '(', 's']\n",
      "['e', 'c', 'u', 'r', 'r', 'e', 'n', 'c', 'e', ' ', 'p', 'l', 'o', 't', ' ']\n",
      "Target: \n",
      "['d', 'e', 'f', ' ', '_', '_', 'i', 'n', 'i', 't', '_', '_', '(', 's', 'e']\n",
      "['c', 'u', 'r', 'r', 'e', 'n', 'c', 'e', ' ', 'p', 'l', 'o', 't', ' ', 'u']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Batch 1\n",
      "Input:\n",
      "['f', ' ', 'n', 'u', 'm', 'b', 'e', 'r', '\\n', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "['g', 't', 'h', '!', ' ', 'F', 'o', 'r', 'm', 'a', 'l', 'l', 'y', ',', ' ']\n",
      "Target: \n",
      "[' ', 'n', 'u', 'm', 'b', 'e', 'r', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "['t', 'h', '!', ' ', 'F', 'o', 'r', 'm', 'a', 'l', 'l', 'y', ',', ' ', 'n']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "max_batch_size = 2\n",
    "max_window_size = 15\n",
    "\n",
    "gen = batch_sliding_window_generator(training_data, max_batch_size, max_window_size)\n",
    "for i in range(2):\n",
    "    inp, target = next(gen)\n",
    "    print(\"Batch {}\".format(i))\n",
    "    print(\"Input:\")\n",
    "    print(*inp, sep=\"\\n\")\n",
    "    print(\"Target: \")\n",
    "    print(*target, sep=\"\\n\")\n",
    "    print(\"~\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many batches we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 180\n",
      " • 1 batch_item of sequence length 7308\n",
      " • 1 batch_item of sequence length 1558\n",
      " • 1 batch_item of sequence length 295\n",
      " • 1 batch_item of sequence length 348\n",
      " • 1 batch_item of sequence length 76\n",
      " • 1 batch_item of sequence length 852\n",
      " • 1 batch_item of sequence length 4845\n",
      " • 1 batch_item of sequence length 87\n",
      " • 1 batch_item of sequence length 36\n",
      " • 1 batch_item of sequence length 174\n",
      " • 1 batch_item of sequence length 587\n",
      " • 1 batch_item of sequence length 2955\n",
      " • 1 batch_item of sequence length 3783\n",
      " • 1 batch_item of sequence length 1518\n",
      " • 1 batch_item of sequence length 1488\n",
      " • 1 batch_item of sequence length 17322\n",
      " • 1 batch_item of sequence length 1523\n",
      " • 1 batch_item of sequence length 6478\n",
      " • 1 batch_item of sequence length 1842\n",
      " • 1 batch_item of sequence length 6775\n",
      " • 1 batch_item of sequence length 160\n",
      " • 1 batch_item of sequence length 3079\n",
      " • 1 batch_item of sequence length 75\n",
      " • 1 batch_item of sequence length 1504\n",
      " • 1 batch_item of sequence length 1800\n",
      " • 1 batch_item of sequence length 77\n",
      " • 1 batch_item of sequence length 3887\n",
      " • 1 batch_item of sequence length 4371\n",
      " • 1 batch_item of sequence length 207\n",
      " • 1 batch_item of sequence length 2195\n",
      " • 1 batch_item of sequence length 9256\n",
      " • 1 batch_item of sequence length 1268\n",
      " • 1 batch_item of sequence length 1284\n",
      " • 1 batch_item of sequence length 1638\n",
      " • 1 batch_item of sequence length 15042\n",
      " • 1 batch_item of sequence length 285\n",
      " • 1 batch_item of sequence length 7984\n",
      " • 1 batch_item of sequence length 262\n",
      " • 1 batch_item of sequence length 431\n",
      " • 1 batch_item of sequence length 421\n",
      " • 1 batch_item of sequence length 1316\n",
      " • 1 batch_item of sequence length 1774\n",
      " • 1 batch_item of sequence length 723\n",
      " • 1 batch_item of sequence length 12869\n",
      " • 1 batch_item of sequence length 732\n",
      " • 1 batch_item of sequence length 8823\n",
      " • 1 batch_item of sequence length 24062\n",
      " • 1 batch_item of sequence length 4085\n",
      " • 1 batch_item of sequence length 1564\n",
      " • 1 batch_item of sequence length 5825\n",
      " • 1 batch_item of sequence length 3938\n",
      " • 1 batch_item of sequence length 391\n",
      " • 1 batch_item of sequence length 845\n",
      " • 1 batch_item of sequence length 443\n",
      " • 1 batch_item of sequence length 2143\n",
      " • 1 batch_item of sequence length 1\n",
      " • 1 batch_item of sequence length 101\n",
      " • 1 batch_item of sequence length 1789\n",
      " • 1 batch_item of sequence length 350\n",
      " • 1 batch_item of sequence length 727\n",
      " • 1 batch_item of sequence length 23\n",
      " • 1 batch_item of sequence length 441\n",
      " • 1 batch_item of sequence length 1021\n",
      " • 1 batch_item of sequence length 12613\n",
      " • 1 batch_item of sequence length 71\n",
      " • 1 batch_item of sequence length 45\n",
      " • 1 batch_item of sequence length 9102\n",
      " • 1 batch_item of sequence length 74\n",
      " • 1 batch_item of sequence length 2509\n",
      " • 1 batch_item of sequence length 2439\n",
      " • 1 batch_item of sequence length 923\n",
      " • 1 batch_item of sequence length 50\n",
      " • 1 batch_item of sequence length 417\n",
      " • 1 batch_item of sequence length 2009\n",
      " • 1 batch_item of sequence length 10014\n",
      " • 1 batch_item of sequence length 3992\n",
      " • 1 batch_item of sequence length 774\n",
      " • 1 batch_item of sequence length 4271\n",
      " • 1 batch_item of sequence length 211\n",
      " • 1 batch_item of sequence length 6415\n",
      " • 1 batch_item of sequence length 1475\n",
      " • 1 batch_item of sequence length 1575\n",
      " • 1 batch_item of sequence length 123\n",
      " • 1 batch_item of sequence length 6065\n",
      " • 1 batch_item of sequence length 4668\n",
      " • 1 batch_item of sequence length 12221\n",
      " • 1 batch_item of sequence length 32\n",
      " • 1 batch_item of sequence length 11875\n",
      " • 1 batch_item of sequence length 5033\n",
      " • 1 batch_item of sequence length 8567\n",
      " • 1 batch_item of sequence length 518\n",
      " • 1 batch_item of sequence length 8280\n",
      " • 1 batch_item of sequence length 515\n",
      " • 1 batch_item of sequence length 2531\n",
      " • 1 batch_item of sequence length 5078\n",
      " • 1 batch_item of sequence length 16178\n",
      " • 1 batch_item of sequence length 1893\n",
      " • 1 batch_item of sequence length 1442\n",
      " • 1 batch_item of sequence length 5850\n",
      " • 1 batch_item of sequence length 687\n",
      " • 1 batch_item of sequence length 1053\n",
      " • 1 batch_item of sequence length 6411\n",
      " • 1 batch_item of sequence length 44925\n",
      " • 1 batch_item of sequence length 2184\n",
      " • 1 batch_item of sequence length 2081\n",
      " • 1 batch_item of sequence length 2847\n",
      " • 1 batch_item of sequence length 225\n",
      " • 1 batch_item of sequence length 965\n",
      " • 1 batch_item of sequence length 508\n",
      " • 1 batch_item of sequence length 1212\n",
      " • 1 batch_item of sequence length 6353\n",
      " • 1 batch_item of sequence length 633\n",
      " • 1 batch_item of sequence length 2799\n",
      " • 1 batch_item of sequence length 22\n",
      " • 1 batch_item of sequence length 6284\n",
      " • 1 batch_item of sequence length 1815\n",
      " • 1 batch_item of sequence length 2171\n",
      " • 1 batch_item of sequence length 4605\n",
      " • 1 batch_item of sequence length 2074\n",
      " • 1 batch_item of sequence length 847\n",
      " • 1 batch_item of sequence length 433\n",
      " • 1 batch_item of sequence length 129\n",
      " • 1 batch_item of sequence length 2376\n",
      " • 1 batch_item of sequence length 1041\n",
      " • 1 batch_item of sequence length 10548\n",
      " • 1 batch_item of sequence length 790\n",
      " • 1 batch_item of sequence length 873\n",
      " • 1 batch_item of sequence length 83\n",
      " • 1 batch_item of sequence length 2920\n",
      " • 1 batch_item of sequence length 809\n",
      " • 1 batch_item of sequence length 5696\n",
      " • 1 batch_item of sequence length 1423\n",
      " • 1 batch_item of sequence length 81\n",
      " • 1 batch_item of sequence length 387\n",
      " • 1 batch_item of sequence length 5241\n",
      " • 1 batch_item of sequence length 675\n",
      " • 1 batch_item of sequence length 1052\n",
      " • 1 batch_item of sequence length 32824\n",
      " • 1 batch_item of sequence length 5335\n",
      " • 1 batch_item of sequence length 5336\n",
      " • 1 batch_item of sequence length 360\n",
      " • 1 batch_item of sequence length 1355\n",
      " • 1 batch_item of sequence length 2933\n",
      " • 1 batch_item of sequence length 2344\n",
      " • 1 batch_item of sequence length 130\n",
      " • 1 batch_item of sequence length 2654\n",
      " • 1 batch_item of sequence length 3163\n",
      " • 1 batch_item of sequence length 3622\n",
      " • 1 batch_item of sequence length 274\n",
      " • 1 batch_item of sequence length 5249\n",
      " • 1 batch_item of sequence length 3306\n",
      " • 1 batch_item of sequence length 221\n",
      " • 1 batch_item of sequence length 16736\n",
      " • 1 batch_item of sequence length 2129\n",
      " • 1 batch_item of sequence length 28\n",
      " • 1 batch_item of sequence length 585\n",
      " • 1 batch_item of sequence length 2083\n",
      " • 1 batch_item of sequence length 420\n",
      " • 1 batch_item of sequence length 4316\n",
      " • 1 batch_item of sequence length 1116\n",
      " • 1 batch_item of sequence length 9637\n",
      " • 1 batch_item of sequence length 2869\n",
      " • 1 batch_item of sequence length 2190\n",
      " • 1 batch_item of sequence length 1161\n",
      " • 1 batch_item of sequence length 14902\n",
      " • 1 batch_item of sequence length 742\n",
      " • 1 batch_item of sequence length 35\n",
      " • 1 batch_item of sequence length 3650\n",
      " • 1 batch_item of sequence length 3916\n",
      " • 1 batch_item of sequence length 749\n",
      " • 1 batch_item of sequence length 2047\n",
      " • 1 batch_item of sequence length 872\n",
      " • 1 batch_item of sequence length 17571\n",
      " • 1 batch_item of sequence length 895\n",
      " • 1 batch_item of sequence length 2680\n",
      " • 1 batch_item of sequence length 867\n",
      " • 1 batch_item of sequence length 3021\n",
      " • 1 batch_item of sequence length 2816\n",
      " • 1 batch_item of sequence length 550\n",
      " • 1 batch_item of sequence length 34\n"
     ]
    }
   ],
   "source": [
    "check_batch_size = 1\n",
    "check_window_size = None\n",
    "\n",
    "gen = batch_sliding_window_generator(training_data, check_batch_size, check_window_size, gen_forever=False)\n",
    "\n",
    "batch_item_sizes = {}\n",
    "batch_sizes = {}\n",
    "batch_counter = 0\n",
    "for inp, target in gen:\n",
    "    batch_counter += 1\n",
    "    batch_size = len(inp[0])\n",
    "    for batch_item in inp:\n",
    "        assert len(batch_item) == batch_size\n",
    "        batch_item_sizes[len(batch_item)] = batch_item_sizes.get(len(batch_item), 0) + 1\n",
    "    batch_sizes[batch_size] = batch_sizes.get(batch_size, 0 ) + 1\n",
    "print(\"Batches: {}\".format(batch_counter))\n",
    "for k, v in batch_item_sizes.items():\n",
    "    print(\" • {} batch_item of sequence length {}\".format(v, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Characters to Tensors\n",
    "\n",
    "Our batch of inputs and outputs must be converted into Tensors for training.\n",
    "\n",
    "We follow the default matrix convention used by the PyTorch comunity.\n",
    "\n",
    "> Tensor’s data will be of size `T x B x *`, where `T` is the length of the longest sequence and `B` is the batch size. \n",
    "\n",
    "To make a batch training example, we join a bunch our sequences of one-hot characters into a matrix of size `(window_size, batch_size, num_chars)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_tensor(char, num_chars=len(CHARACTERS)):\n",
    "    tensor = torch.zeros(1, 1, num_chars, device=device)\n",
    "    tensor[0][0][CHAR2INT[char]] = 1\n",
    "    return tensor\n",
    "\n",
    "def charseq_to_tensor(charseq, num_chars=len(CHARACTERS)):\n",
    "    window_size = len(charseq)\n",
    "    tensor = torch.zeros(window_size, 1, num_chars, device=device)\n",
    "    for seq_idx, seq_item in enumerate(charseq):\n",
    "        tensor[seq_idx][0][CHAR2INT[seq_item]] = 1\n",
    "    return tensor\n",
    "\n",
    "def charseqs_to_tensor(charseqs, num_chars=len(CHARACTERS)):\n",
    "    batch_size = len(charseqs)\n",
    "    window_size = max([len(v) for v in charseqs])\n",
    "    assert all(len(v) == window_size for v in charseqs)\n",
    "    \n",
    "    tensor = torch.zeros(window_size, batch_size, num_chars, device=device)\n",
    "    \n",
    "    for batch_elm_idx, batch_item in enumerate(charseqs):\n",
    "        for seq_idx, seq_item in enumerate(batch_item):\n",
    "            tensor[seq_idx][batch_elm_idx][CHAR2INT[seq_item]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor inputs and outputs for the generator function we defined earlier are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_batch_size: 2, max_window_size: 15\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[' ', 'a', 'n', 'd', ' ', 'y', ',', ' ', 'c', 'a', 'l', 'c', 'u', 'l', 'a']\n",
      "['h', ' ', 't', 'i', 'm', 'e', ' ', 's', 'e', 'r', 'i', 'e', 's', '.', '\\n']\n",
      "torch.Size([15, 2, 100])\n",
      "['a', 'n', 'd', ' ', 'y', ',', ' ', 'c', 'a', 'l', 'c', 'u', 'l', 'a', 't']\n",
      "[' ', 't', 'i', 'm', 'e', ' ', 's', 'e', 'r', 'i', 'e', 's', '.', '\\n', ' ']\n",
      "torch.Size([15, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "gen = batch_sliding_window_generator(training_data, max_batch_size, max_window_size)\n",
    "\n",
    "inp_val, target_val = next(gen)\n",
    "inp_tensor = charseqs_to_tensor(inp_val)\n",
    "target_tensor = charseqs_to_tensor(target_val)\n",
    "\n",
    "print(\"max_batch_size: {}, max_window_size: {}\".format(max_batch_size, max_window_size))\n",
    "print(\"~\" * 25)\n",
    "print(*inp_val, sep=\"\\n\")\n",
    "print(inp_tensor.size())\n",
    "#_, idxs = inp_tensor.topk(1)\n",
    "print(*target_val, sep=\"\\n\")\n",
    "print(target_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Network\n",
    "\n",
    "We are going to create our recurrent neural network. Rather than using our own pure recurrent neural network defined in the last notebook, we will be using the recurrent layers provided by PyTorch.\n",
    "\n",
    "* **RNN**: [Recurrent Neural Network](https://pytorch.org/docs/stable/nn.html#rnn)\n",
    "* **GRU**: [Gated Recurrent Unit](https://pytorch.org/docs/stable/nn.html#gru)\n",
    "* **LSTM**: [Long Short-Term Memory](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "\n",
    "Our model is composed of the following:\n",
    "0. Recurrent Layer (RNN/GRU/LSTM)\n",
    "0. Linear Layer\n",
    "0. Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size,\n",
    "                 hidden_size=128, recurrent_type=\"LSTM\", recurrent_layers=1, recurrent_dropout=0):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.recurrent_type = recurrent_type.upper()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recurrent_layers = recurrent_layers\n",
    "        self.recurrent_dropoout = recurrent_dropout\n",
    "        \n",
    "        rn_kwargs = {\n",
    "            \"input_size\": input_size,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": recurrent_layers,\n",
    "            \"dropout\": recurrent_dropout,\n",
    "        }\n",
    "        \n",
    "        if self.recurrent_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(**rn_kwargs)\n",
    "        elif self.recurrent_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(**rn_kwargs)\n",
    "        elif self.recurrent_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(**rn_kwargs)\n",
    "        else:\n",
    "            raise \"Invalid recurrent layer type: {}\".format(recurrent_type)\n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, inp_val, hidden):\n",
    "        for_decoder, hidden = self.rnn(inp_val, hidden)\n",
    "        for_softmax = self.decoder(for_decoder)\n",
    "        output = self.softmax(for_softmax)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.recurrent_type == \"LSTM\":\n",
    "            return (\n",
    "                torch.zeros(self.recurrent_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.recurrent_layers, batch_size, self.hidden_size, device=device)\n",
    "            )\n",
    "        return torch.zeros(self.recurrent_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize this network with values appropriate for the character prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(CHARACTERS)\n",
    "batch_size = 4\n",
    "\n",
    "char_rnn = CharRNN(n_chars, n_chars)\n",
    "if HAS_CUDA:\n",
    "    char_rnn.cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before each batch, the RNN's hidden state should be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([20, 2, 100])\n",
      "Output: torch.Size([20, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "max_window_size = 20\n",
    "gen = batch_sliding_window_generator(\n",
    "    training_data, max_batch_size, max_window_size, gen_forever=False)\n",
    "inp_charseq, target_charseq = next(gen)\n",
    "inp_val = charseqs_to_tensor(inp_charseq)\n",
    "_, batch_size, _ = inp_val.size()\n",
    "hidden = char_rnn.init_hidden(batch_size)\n",
    "\n",
    "output, hidden = char_rnn(inp_val, hidden)\n",
    "print(\"Input:\", inp_val.size())\n",
    "print(\"Output:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Train our Model\n",
    "\n",
    "We will create a helper function to convert the network predicted output back to a human readable character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: owwwwwwwwwwwwowKoKow\n",
      "Batch 1: wwwwKwwvoooooooooooo\n"
     ]
    }
   ],
   "source": [
    "def readable_from_output(output):\n",
    "    \"\"\"Convert the network output predictions back to readable values.\"\"\"\n",
    "    _, top_i = output.topk(1)\n",
    "    window_size, batch_size, _ = top_i.size()\n",
    "    batches_chars = []\n",
    "    for batch_ele_idx in range(batch_size):\n",
    "        batch_chars = []\n",
    "        for seq_ele_idx in range(window_size):\n",
    "            char_idx = top_i[seq_ele_idx][batch_ele_idx].item()\n",
    "            char = INT2CHAR[char_idx], char_idx\n",
    "            batch_chars.append(char)\n",
    "        batches_chars.append(batch_chars)\n",
    "    return batches_chars\n",
    "\n",
    "def pretty_print_output(batches_chars):\n",
    "    for seq_idx, seq in enumerate(batches_chars):\n",
    "        print(\"Batch {}:\".format(seq_idx), \"\".join([char for char, _ in seq]))\n",
    "\n",
    "batches_chars = readable_from_output(output)\n",
    "pretty_print_output(batches_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the network some examples, have it make predictions, and then inform the network when the predictions are correct.\n",
    "We have a classification problem, so Negative Log Likelihood loss is appropriate.\n",
    "\n",
    "For classifing input into `C` number of classes, the following loss functions are useful:\n",
    "\n",
    "* **Cross Entropy Loss**: [CrossEntropyLoss docs](https://pytorch.org/docs/stable/nn.html#crossentropyloss)\n",
    "* **Negative Log Likelihood Loss**: [NLLLoss docs](https://pytorch.org/docs/stable/nn.html#nllloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training loop will:\n",
    "0. Create input and target tensors\n",
    "0. Initialize a zerored hidden state\n",
    "0. Read each letter in and keep the hidden state for the next letter\n",
    "0. Compare the final output to the target\n",
    "0. Back-propagate\n",
    "0. Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(target_tensor, input_tensor, optimizer):\n",
    "    window_size, batch_size, _ = input_tensor.size()\n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    \n",
    "    # Set model gradients to zero\n",
    "    char_rnn.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    correct = []\n",
    "    for i in range(window_size):\n",
    "        output, hidden = char_rnn(input_tensor.narrow(0, i, 1), hidden)\n",
    "        _, loss_output_tensor = target_tensor[i].topk(1)\n",
    "        # diagnose training issues\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        loss += criterion(output.view(batch_size, -1), loss_output_tensor.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.item() / window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We want to also evaluate the effectiveness of our model on a validation set, without updating the model.\n",
    "\n",
    "This is done in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target_tensor, input_tensor):\n",
    "    window_size, batch_size, _ = input_tensor.size()\n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    \n",
    "    # Set model gradients to zero\n",
    "    char_rnn.zero_grad()\n",
    "    loss = 0\n",
    "    match = []\n",
    "    \n",
    "    hidden = char_rnn.init_hidden(batch_size)\n",
    "    for i in range(window_size):\n",
    "        output, hidden = char_rnn(input_tensor.narrow(0, i, 1), hidden)\n",
    "        _, target_character = target_tensor[i].topk(1)\n",
    "        _, predict_character = output.topk(1)\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        for batch_match in target_character.view(-1) == predict_character.view(-1):\n",
    "            match.append(batch_match.item())\n",
    "        \n",
    "        loss += criterion(output.view(batch_size, -1), target_character.view(-1))\n",
    "    \n",
    "    # loss.backward()\n",
    "    accuracy = mean(match)\n",
    "    return accuracy, loss.item() / window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Model\n",
    "\n",
    "Let's train our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required variables\n",
    "n_chars = len(CHARACTERS)\n",
    "max_batch_size = 1\n",
    "char_rnn = CharRNN(n_chars, n_chars, recurrent_type=\"LSTM\")\n",
    "if HAS_CUDA:\n",
    "    char_rnn.cuda(device)\n",
    "learning_rate = 0.001\n",
    "char_rnn_optimizer = torch.optim.Adam(char_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Helper function\n",
    "def time_since(since):\n",
    "    delta = time() - since\n",
    "    sec = int(delta)\n",
    "    mins = floor(sec / 60)\n",
    "    sec -= mins * 60\n",
    "    return \"{}m {:0>2d}s\".format(mins, sec)\n",
    "\n",
    "# Keep track of all the trainng that is done\n",
    "train_epoch_losses = []\n",
    "eval_epoch_losses = []\n",
    "batch_iter_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable which can change in-between trainng\n",
    "n_epochs = 100\n",
    "print_every_train = 47\n",
    "# print_every_train = 1\n",
    "print_every_epoch = 1\n",
    "max_window_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 0.0%: iter     0 (0m 04s) | Train Loss: 4.61060\n",
      "Epoch 0 0.0%: iter    47 (1m 52s) | Train Loss: 3.34160\n",
      "Epoch 0 0.0%: iter    94 (3m 27s) | Train Loss: 3.22136\n",
      "Epoch 0 0.0%: iter   141 (5m 37s) | Train Loss: 3.56943\n",
      "> Epoch 1 0.0%: (7m 22s) | Train Loss: 3.62499, Mean Eval Loss: 3.27614, Mean Acc: 0.21\n",
      "Epoch 1 1.0%: iter   188 (7m 32s) | Train Loss: 3.57865\n",
      "Epoch 1 1.0%: iter   235 (9m 24s) | Train Loss: 4.86477\n",
      "Epoch 1 1.0%: iter   282 (11m 41s) | Train Loss: 3.13480\n",
      "Epoch 1 1.0%: iter   329 (13m 13s) | Train Loss: 2.92207\n",
      "> Epoch 2 1.0%: (14m 44s) | Train Loss: 3.55555, Mean Eval Loss: 3.10549, Mean Acc: 0.23\n",
      "Epoch 2 2.0%: iter   376 (15m 14s) | Train Loss: 3.09847\n",
      "Epoch 2 2.0%: iter   423 (16m 58s) | Train Loss: 2.84394\n",
      "Epoch 2 2.0%: iter   470 (19m 15s) | Train Loss: 3.28965\n",
      "Epoch 2 2.0%: iter   517 (20m 53s) | Train Loss: 2.95407\n",
      "> Epoch 3 2.0%: (22m 07s) | Train Loss: 3.40122, Mean Eval Loss: 2.97849, Mean Acc: 0.26\n",
      "Epoch 3 3.0%: iter   564 (22m 51s) | Train Loss: 2.80935\n",
      "Epoch 3 3.0%: iter   611 (24m 31s) | Train Loss: 3.18164\n",
      "Epoch 3 3.0%: iter   658 (26m 51s) | Train Loss: 2.45327\n",
      "Epoch 3 3.0%: iter   705 (28m 40s) | Train Loss: 3.27182\n",
      "> Epoch 4 3.0%: (29m 30s) | Train Loss: 3.18474, Mean Eval Loss: 2.82728, Mean Acc: 0.29\n",
      "Epoch 4 4.0%: iter   752 (30m 29s) | Train Loss: 2.75438\n",
      "Epoch 4 4.0%: iter   799 (32m 12s) | Train Loss: 2.39401\n",
      "Epoch 4 4.0%: iter   846 (34m 26s) | Train Loss: 2.60892\n",
      "Epoch 4 4.0%: iter   893 (36m 23s) | Train Loss: 2.60932\n",
      "> Epoch 5 4.0%: (36m 53s) | Train Loss: 3.03896, Mean Eval Loss: 2.73977, Mean Acc: 0.31\n",
      "Epoch 5 5.0%: iter   940 (38m 10s) | Train Loss: 3.41123\n",
      "Epoch 5 5.0%: iter   987 (40m 01s) | Train Loss: 2.24470\n",
      "Epoch 5 5.0%: iter  1034 (42m 00s) | Train Loss: 2.39627\n",
      "> Epoch 6 5.0%: (44m 16s) | Train Loss: 2.91289, Mean Eval Loss: 2.66582, Mean Acc: 0.32\n",
      "Epoch 6 6.0%: iter  1081 (44m 22s) | Train Loss: 3.29474\n",
      "Epoch 6 6.0%: iter  1128 (46m 10s) | Train Loss: 2.94591\n",
      "Epoch 6 6.0%: iter  1175 (47m 55s) | Train Loss: 2.36805\n",
      "Epoch 6 6.0%: iter  1222 (49m 56s) | Train Loss: 2.55709\n",
      "> Epoch 7 6.0%: (51m 39s) | Train Loss: 2.83074, Mean Eval Loss: 2.59748, Mean Acc: 0.35\n",
      "Epoch 7 7.0%: iter  1269 (51m 50s) | Train Loss: 2.76858\n",
      "Epoch 7 7.0%: iter  1316 (53m 42s) | Train Loss: 3.08353\n",
      "Epoch 7 7.0%: iter  1363 (56m 01s) | Train Loss: 3.36986\n",
      "Epoch 7 7.0%: iter  1410 (57m 34s) | Train Loss: 2.40960\n",
      "> Epoch 8 7.0%: (59m 03s) | Train Loss: 2.74244, Mean Eval Loss: 2.54743, Mean Acc: 0.36\n",
      "Epoch 8 8.0%: iter  1457 (59m 37s) | Train Loss: 2.26466\n",
      "Epoch 8 8.0%: iter  1504 (61m 17s) | Train Loss: 2.82193\n",
      "Epoch 8 8.0%: iter  1551 (63m 35s) | Train Loss: 2.78439\n",
      "Epoch 8 8.0%: iter  1598 (65m 15s) | Train Loss: 2.75213\n",
      "> Epoch 9 8.0%: (66m 27s) | Train Loss: 2.70394, Mean Eval Loss: 2.51573, Mean Acc: 0.37\n",
      "Epoch 9 9.0%: iter  1645 (67m 11s) | Train Loss: 3.11009\n",
      "Epoch 9 9.0%: iter  1692 (68m 51s) | Train Loss: 2.77274\n",
      "Epoch 9 9.0%: iter  1739 (71m 12s) | Train Loss: 3.45718\n",
      "Epoch 9 9.0%: iter  1786 (73m 00s) | Train Loss: 2.96892\n",
      "> Epoch 10 9.0%: (73m 50s) | Train Loss: 2.63805, Mean Eval Loss: 2.47569, Mean Acc: 0.37\n",
      "Epoch 10 10.0%: iter  1833 (74m 50s) | Train Loss: 2.77693\n",
      "Epoch 10 10.0%: iter  1880 (76m 33s) | Train Loss: 2.81655\n",
      "Epoch 10 10.0%: iter  1927 (78m 46s) | Train Loss: 2.45724\n",
      "Epoch 10 10.0%: iter  1974 (80m 45s) | Train Loss: 2.32384\n",
      "> Epoch 11 10.0%: (81m 13s) | Train Loss: 2.61698, Mean Eval Loss: 2.43424, Mean Acc: 0.38\n",
      "Epoch 11 11.0%: iter  2021 (82m 32s) | Train Loss: 2.52588\n",
      "Epoch 11 11.0%: iter  2068 (84m 24s) | Train Loss: 2.11550\n",
      "Epoch 11 11.0%: iter  2115 (86m 21s) | Train Loss: 2.75516\n",
      "> Epoch 12 11.0%: (88m 37s) | Train Loss: 2.50951, Mean Eval Loss: 2.39895, Mean Acc: 0.38\n",
      "Epoch 12 12.0%: iter  2162 (88m 43s) | Train Loss: 2.39815\n",
      "Epoch 12 12.0%: iter  2209 (90m 34s) | Train Loss: 2.30970\n",
      "Epoch 12 12.0%: iter  2256 (92m 17s) | Train Loss: 2.30741\n",
      "Epoch 12 12.0%: iter  2303 (94m 18s) | Train Loss: 2.58183\n",
      "> Epoch 13 12.0%: (96m 00s) | Train Loss: 2.48816, Mean Eval Loss: 2.37624, Mean Acc: 0.39\n",
      "Epoch 13 13.0%: iter  2350 (96m 11s) | Train Loss: 2.16942\n",
      "Epoch 13 13.0%: iter  2397 (98m 04s) | Train Loss: 2.42987\n",
      "Epoch 13 13.0%: iter  2444 (100m 23s) | Train Loss: 1.95651\n",
      "Epoch 13 13.0%: iter  2491 (101m 54s) | Train Loss: 3.06680\n",
      "> Epoch 14 13.0%: (103m 23s) | Train Loss: 2.39681, Mean Eval Loss: 2.33831, Mean Acc: 0.39\n",
      "Epoch 14 14.0%: iter  2538 (103m 59s) | Train Loss: 2.35650\n",
      "Epoch 14 14.0%: iter  2585 (105m 37s) | Train Loss: 2.68807\n",
      "Epoch 14 14.0%: iter  2632 (107m 57s) | Train Loss: 2.33680\n",
      "Epoch 14 14.0%: iter  2679 (109m 36s) | Train Loss: 2.61005\n",
      "> Epoch 15 14.0%: (110m 47s) | Train Loss: 2.36696, Mean Eval Loss: 2.31767, Mean Acc: 0.40\n",
      "Epoch 15 15.0%: iter  2726 (111m 33s) | Train Loss: 2.29460\n",
      "Epoch 15 15.0%: iter  2773 (113m 12s) | Train Loss: 1.87866\n",
      "Epoch 15 15.0%: iter  2820 (115m 32s) | Train Loss: 2.81660\n",
      "Epoch 15 15.0%: iter  2867 (117m 23s) | Train Loss: 2.16544\n",
      "> Epoch 16 15.0%: (118m 10s) | Train Loss: 2.29753, Mean Eval Loss: 2.27694, Mean Acc: 0.41\n",
      "Epoch 16 16.0%: iter  2914 (119m 20s) | Train Loss: 2.04294\n",
      "Epoch 16 16.0%: iter  2961 (120m 55s) | Train Loss: 1.93459\n",
      "Epoch 16 16.0%: iter  3008 (123m 09s) | Train Loss: 1.76616\n",
      "Epoch 16 16.0%: iter  3055 (125m 06s) | Train Loss: 2.47737\n",
      "> Epoch 17 16.0%: (125m 34s) | Train Loss: 2.26416, Mean Eval Loss: 2.26516, Mean Acc: 0.41\n",
      "Epoch 17 17.0%: iter  3102 (126m 53s) | Train Loss: 2.85781\n",
      "Epoch 17 17.0%: iter  3149 (128m 51s) | Train Loss: 1.99275\n",
      "Epoch 17 17.0%: iter  3196 (130m 43s) | Train Loss: 2.12186\n",
      "> Epoch 18 17.0%: (132m 58s) | Train Loss: 2.23896, Mean Eval Loss: 2.22395, Mean Acc: 0.42\n",
      "Epoch 18 18.0%: iter  3243 (133m 04s) | Train Loss: 2.55959\n",
      "Epoch 18 18.0%: iter  3290 (134m 58s) | Train Loss: 1.79316\n",
      "Epoch 18 18.0%: iter  3337 (136m 40s) | Train Loss: 2.61725\n",
      "Epoch 18 18.0%: iter  3384 (138m 41s) | Train Loss: 2.58496\n",
      "> Epoch 19 18.0%: (140m 22s) | Train Loss: 2.21498, Mean Eval Loss: 2.20611, Mean Acc: 0.43\n",
      "Epoch 19 19.0%: iter  3431 (140m 35s) | Train Loss: 1.97582\n",
      "Epoch 19 19.0%: iter  3478 (142m 26s) | Train Loss: 2.76264\n",
      "Epoch 19 19.0%: iter  3525 (144m 47s) | Train Loss: 2.08032\n",
      "Epoch 19 19.0%: iter  3572 (146m 28s) | Train Loss: 2.22041\n",
      "> Epoch 20 19.0%: (147m 46s) | Train Loss: 2.16693, Mean Eval Loss: 2.18127, Mean Acc: 0.43\n",
      "Epoch 20 20.0%: iter  3619 (148m 26s) | Train Loss: 2.28100\n",
      "Epoch 20 20.0%: iter  3666 (150m 06s) | Train Loss: 2.55669\n",
      "Epoch 20 20.0%: iter  3713 (152m 20s) | Train Loss: 2.18899\n",
      "Epoch 20 20.0%: iter  3760 (154m 05s) | Train Loss: 1.93524\n",
      "> Epoch 21 20.0%: (155m 10s) | Train Loss: 2.13575, Mean Eval Loss: 2.16235, Mean Acc: 0.44\n",
      "Epoch 21 21.0%: iter  3807 (156m 00s) | Train Loss: 2.08258\n",
      "Epoch 21 21.0%: iter  3854 (157m 42s) | Train Loss: 2.29310\n",
      "Epoch 21 21.0%: iter  3901 (159m 56s) | Train Loss: 2.14011\n",
      "Epoch 21 21.0%: iter  3948 (161m 49s) | Train Loss: 2.21619\n",
      "> Epoch 22 21.0%: (162m 34s) | Train Loss: 2.09274, Mean Eval Loss: 2.14657, Mean Acc: 0.44\n",
      "Epoch 22 22.0%: iter  3995 (163m 44s) | Train Loss: 2.07892\n",
      "Epoch 22 22.0%: iter  4042 (165m 19s) | Train Loss: 1.93795\n",
      "Epoch 22 22.0%: iter  4089 (167m 33s) | Train Loss: 2.03068\n",
      "Epoch 22 22.0%: iter  4136 (169m 32s) | Train Loss: 1.99440\n",
      "> Epoch 23 22.0%: (169m 58s) | Train Loss: 2.07916, Mean Eval Loss: 2.12595, Mean Acc: 0.45\n",
      "Epoch 23 23.0%: iter  4183 (171m 25s) | Train Loss: 1.60702\n",
      "Epoch 23 23.0%: iter  4230 (173m 15s) | Train Loss: 1.51914\n",
      "Epoch 23 23.0%: iter  4277 (175m 28s) | Train Loss: 1.48878\n",
      "> Epoch 24 23.0%: (177m 21s) | Train Loss: 2.05047, Mean Eval Loss: 2.11264, Mean Acc: 0.45\n",
      "Epoch 24 24.0%: iter  4324 (177m 28s) | Train Loss: 2.99868\n",
      "Epoch 24 24.0%: iter  4371 (179m 22s) | Train Loss: 2.44479\n",
      "Epoch 24 24.0%: iter  4418 (181m 06s) | Train Loss: 1.92171\n",
      "Epoch 24 24.0%: iter  4465 (183m 05s) | Train Loss: 1.86646\n",
      "> Epoch 25 24.0%: (184m 45s) | Train Loss: 1.99742, Mean Eval Loss: 2.10097, Mean Acc: 0.46\n",
      "Epoch 25 25.0%: iter  4512 (185m 00s) | Train Loss: 1.75819\n",
      "Epoch 25 25.0%: iter  4559 (186m 49s) | Train Loss: 2.26414\n",
      "Epoch 25 25.0%: iter  4606 (189m 10s) | Train Loss: 2.16468\n",
      "Epoch 25 25.0%: iter  4653 (190m 52s) | Train Loss: 2.40266\n",
      "> Epoch 26 25.0%: (192m 08s) | Train Loss: 1.99562, Mean Eval Loss: 2.07777, Mean Acc: 0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 26.0%: iter  4700 (192m 48s) | Train Loss: 2.32926\n",
      "Epoch 26 26.0%: iter  4747 (194m 29s) | Train Loss: 2.19069\n",
      "Epoch 26 26.0%: iter  4794 (196m 46s) | Train Loss: 2.30727\n",
      "Epoch 26 26.0%: iter  4841 (198m 29s) | Train Loss: 1.58768\n",
      "> Epoch 27 26.0%: (199m 32s) | Train Loss: 1.95596, Mean Eval Loss: 2.05868, Mean Acc: 0.46\n",
      "Epoch 27 27.0%: iter  4888 (200m 21s) | Train Loss: 1.51783\n",
      "Epoch 27 27.0%: iter  4935 (202m 06s) | Train Loss: 1.70929\n",
      "Epoch 27 27.0%: iter  4982 (204m 19s) | Train Loss: 1.67823\n",
      "Epoch 27 27.0%: iter  5029 (206m 10s) | Train Loss: 2.22553\n",
      "> Epoch 28 27.0%: (206m 55s) | Train Loss: 1.96751, Mean Eval Loss: 2.05220, Mean Acc: 0.47\n",
      "Epoch 28 28.0%: iter  5076 (208m 10s) | Train Loss: 1.97790\n",
      "Epoch 28 28.0%: iter  5123 (209m 44s) | Train Loss: 1.73230\n",
      "Epoch 28 28.0%: iter  5170 (211m 57s) | Train Loss: 1.69143\n",
      "Epoch 28 28.0%: iter  5217 (213m 54s) | Train Loss: 2.03778\n",
      "> Epoch 29 28.0%: (214m 18s) | Train Loss: 1.93866, Mean Eval Loss: 2.04860, Mean Acc: 0.47\n",
      "Epoch 29 29.0%: iter  5264 (215m 46s) | Train Loss: 1.94740\n",
      "Epoch 29 29.0%: iter  5311 (217m 40s) | Train Loss: 1.64771\n",
      "Epoch 29 29.0%: iter  5358 (219m 51s) | Train Loss: 1.65642\n",
      "> Epoch 30 29.0%: (221m 40s) | Train Loss: 1.93048, Mean Eval Loss: 2.02077, Mean Acc: 0.48\n",
      "Epoch 30 30.0%: iter  5405 (221m 47s) | Train Loss: 2.15601\n",
      "Epoch 30 30.0%: iter  5452 (223m 41s) | Train Loss: 2.11877\n",
      "Epoch 30 30.0%: iter  5499 (225m 25s) | Train Loss: 1.99104\n",
      "Epoch 30 30.0%: iter  5546 (227m 25s) | Train Loss: 1.86745\n",
      "> Epoch 31 30.0%: (229m 03s) | Train Loss: 1.90147, Mean Eval Loss: 2.01345, Mean Acc: 0.48\n",
      "Epoch 31 31.0%: iter  5593 (229m 19s) | Train Loss: 2.49254\n",
      "Epoch 31 31.0%: iter  5640 (231m 07s) | Train Loss: 1.39218\n",
      "Epoch 31 31.0%: iter  5687 (233m 28s) | Train Loss: 1.82645\n",
      "Epoch 31 31.0%: iter  5734 (235m 09s) | Train Loss: 2.33400\n",
      "> Epoch 32 31.0%: (236m 25s) | Train Loss: 1.88479, Mean Eval Loss: 1.99741, Mean Acc: 0.48\n",
      "Epoch 32 32.0%: iter  5781 (237m 07s) | Train Loss: 1.33243\n",
      "Epoch 32 32.0%: iter  5828 (238m 47s) | Train Loss: 1.28413\n",
      "Epoch 32 32.0%: iter  5875 (241m 04s) | Train Loss: 1.82070\n",
      "Epoch 32 32.0%: iter  5922 (242m 46s) | Train Loss: 1.86432\n",
      "> Epoch 33 32.0%: (243m 47s) | Train Loss: 1.87562, Mean Eval Loss: 1.99526, Mean Acc: 0.48\n",
      "Epoch 33 33.0%: iter  5969 (244m 39s) | Train Loss: 1.99754\n",
      "Epoch 33 33.0%: iter  6016 (246m 22s) | Train Loss: 2.18998\n",
      "Epoch 33 33.0%: iter  6063 (248m 35s) | Train Loss: 2.15533\n",
      "Epoch 33 33.0%: iter  6110 (250m 27s) | Train Loss: 2.04919\n",
      "> Epoch 34 33.0%: (251m 10s) | Train Loss: 1.88884, Mean Eval Loss: 1.97598, Mean Acc: 0.49\n",
      "Epoch 34 34.0%: iter  6157 (252m 26s) | Train Loss: 2.15831\n",
      "Epoch 34 34.0%: iter  6204 (254m 02s) | Train Loss: 1.66716\n",
      "Epoch 34 34.0%: iter  6251 (256m 13s) | Train Loss: 1.66409\n",
      "Epoch 34 34.0%: iter  6298 (258m 09s) | Train Loss: 2.14768\n",
      "> Epoch 35 34.0%: (258m 32s) | Train Loss: 1.86170, Mean Eval Loss: 1.98549, Mean Acc: 0.48\n",
      "Epoch 35 35.0%: iter  6345 (260m 06s) | Train Loss: 1.76348\n",
      "Epoch 35 35.0%: iter  6392 (261m 55s) | Train Loss: 1.53675\n",
      "Epoch 35 35.0%: iter  6439 (264m 08s) | Train Loss: 1.39478\n",
      "> Epoch 36 35.0%: (265m 54s) | Train Loss: 1.84463, Mean Eval Loss: 1.95854, Mean Acc: 0.49\n",
      "Epoch 36 36.0%: iter  6486 (266m 04s) | Train Loss: 1.71527\n",
      "Epoch 36 36.0%: iter  6533 (267m 55s) | Train Loss: 2.03440\n",
      "Epoch 36 36.0%: iter  6580 (269m 40s) | Train Loss: 1.90002\n",
      "Epoch 36 36.0%: iter  6627 (271m 42s) | Train Loss: 2.29275\n",
      "> Epoch 37 36.0%: (273m 17s) | Train Loss: 1.86202, Mean Eval Loss: 1.95478, Mean Acc: 0.49\n",
      "Epoch 37 37.0%: iter  6674 (273m 34s) | Train Loss: 1.81449\n",
      "Epoch 37 37.0%: iter  6721 (275m 21s) | Train Loss: 1.75525\n",
      "Epoch 37 37.0%: iter  6768 (277m 42s) | Train Loss: 1.94429\n",
      "Epoch 37 37.0%: iter  6815 (279m 23s) | Train Loss: 1.82732\n",
      "> Epoch 38 37.0%: (280m 39s) | Train Loss: 1.85307, Mean Eval Loss: 1.94210, Mean Acc: 0.50\n",
      "Epoch 38 38.0%: iter  6862 (281m 20s) | Train Loss: 2.62128\n",
      "Epoch 38 38.0%: iter  6909 (283m 01s) | Train Loss: 1.49748\n",
      "Epoch 38 38.0%: iter  6956 (285m 18s) | Train Loss: 1.69721\n",
      "Epoch 38 38.0%: iter  7003 (287m 00s) | Train Loss: 2.40792\n",
      "> Epoch 39 38.0%: (288m 01s) | Train Loss: 1.83734, Mean Eval Loss: 1.94296, Mean Acc: 0.50\n",
      "Epoch 39 39.0%: iter  7050 (288m 58s) | Train Loss: 2.11841\n",
      "Epoch 39 39.0%: iter  7097 (290m 39s) | Train Loss: 1.89986\n",
      "Epoch 39 39.0%: iter  7144 (292m 55s) | Train Loss: 1.60767\n",
      "Epoch 39 39.0%: iter  7191 (294m 40s) | Train Loss: 2.29027\n",
      "> Epoch 40 39.0%: (295m 23s) | Train Loss: 1.81756, Mean Eval Loss: 1.93180, Mean Acc: 0.50\n",
      "Epoch 40 40.0%: iter  7238 (296m 39s) | Train Loss: 1.76173\n",
      "Epoch 40 40.0%: iter  7285 (298m 22s) | Train Loss: 1.54626\n",
      "Epoch 40 40.0%: iter  7332 (300m 25s) | Train Loss: 2.49450\n",
      "Epoch 40 40.0%: iter  7379 (302m 21s) | Train Loss: 1.79920\n",
      "> Epoch 41 40.0%: (302m 45s) | Train Loss: 1.79920, Mean Eval Loss: 1.92314, Mean Acc: 0.50\n",
      "Epoch 41 41.0%: iter  7426 (304m 34s) | Train Loss: 1.78823\n",
      "Epoch 41 41.0%: iter  7473 (306m 09s) | Train Loss: 1.44963\n",
      "Epoch 41 41.0%: iter  7520 (308m 22s) | Train Loss: 2.36623\n",
      "> Epoch 42 41.0%: (310m 07s) | Train Loss: 1.78479, Mean Eval Loss: 1.91640, Mean Acc: 0.50\n",
      "Epoch 42 42.0%: iter  7567 (310m 17s) | Train Loss: 2.22984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fdf6965e0a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharseqs_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_char_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpred_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_rnn_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_iter_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d179d3f957d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(target_tensor, input_tensor, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_output_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# diagnose training issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/catamaran/genpyseq/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0760d0ab1457>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp_val, hidden)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mfor_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mfor_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/catamaran/genpyseq/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/catamaran/genpyseq/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    # This generator will terminate when it finishes the file.\n",
    "    gen = batch_sliding_window_generator(\n",
    "        training_data, max_batch_size, max_window_size, gen_forever=False)\n",
    "    for input_char_seq, target_char_seq in gen:\n",
    "        # diagnose training issues\n",
    "        #import pdb; pdb.set_trace()\n",
    "    \n",
    "        input_tensor = charseqs_to_tensor(input_char_seq)\n",
    "        target_tensor = charseqs_to_tensor(target_char_seq)\n",
    "        \n",
    "        pred_output, train_loss = train(target_tensor, input_tensor, char_rnn_optimizer)\n",
    "\n",
    "        if batch_iter_idx % print_every_train == 0:            \n",
    "            print(\"Epoch {:1d} {:.1f}%: iter {:5d} ({}) | Train Loss: {:.5f}\".format(\n",
    "                len(train_epoch_losses), epoch_idx/n_epochs * 100, batch_iter_idx,\n",
    "                time_since(start), train_loss))\n",
    "        batch_iter_idx += 1\n",
    "\n",
    "    # Add current loss avg to list of losses after each epoch\n",
    "    train_epoch_losses.append(train_loss)\n",
    "\n",
    "    # Run our model on our validation data set to determine validation loss and accuracy\n",
    "    gen = batch_sliding_window_generator(\n",
    "        validation_data, 1, max_window_size, gen_forever=False)\n",
    "    \n",
    "    eval_accuracies = []\n",
    "    eval_losses = []\n",
    "    for input_char_seqs, target_char_seqs in gen:\n",
    "        input_tensor = charseqs_to_tensor(input_char_seqs)\n",
    "        target_tensor = charseqs_to_tensor(target_char_seqs)\n",
    "        eval_accuracy, eval_loss = evaluate(target_tensor, input_tensor)\n",
    "        eval_accuracies.append(eval_accuracy)\n",
    "        eval_losses.append(eval_loss)\n",
    "\n",
    "    mean_eval_loss = mean(eval_losses)\n",
    "    mean_eval_acc = mean(eval_accuracies)\n",
    "\n",
    "    eval_epoch_losses.append(mean_eval_loss)\n",
    "    \n",
    "    if len(train_epoch_losses) % print_every_epoch == 0:\n",
    "        print(\"> Epoch {:1d} {:.1f}%: ({}) | Train Loss: {:.5f}, Mean Eval Loss: {:.5f}, Mean Acc: {:.2f}\".format(\n",
    "            len(train_epoch_losses), epoch_idx/n_epochs * 100,\n",
    "            time_since(start), train_loss, mean_eval_loss, mean_eval_acc))\n",
    "    torch.save(char_rnn.state_dict(), \"./models/char-rnn-epoch-{}-loss-{}.hdf5\".format(\n",
    "        len(train_epoch_losses), mean_eval_loss))\n",
    "\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "#model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Training Process\n",
    "Plotting the historical loss from train_epoch_losses show how well the network is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f20d479fb00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5), dpi=80)\n",
    "plt.title(\"Batching Char-RNN Training Loss over Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Negative Log Likelihood\")\n",
    "plt.plot(train_epoch_losses, color=\"green\", label=\"Training Loss\")\n",
    "plt.plot(eval_epoch_losses, color=\"orange\", label=\"Validation Loss\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using User Specified Input\n",
    "Let's test this ourselves by supplying our own Python source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, prime_str=FILE_START, print_output=True, max_gen_len=1000, temperature=None):\n",
    "    hidden = rnn.init_hidden(1)\n",
    "    if not prime_str.startswith(FILE_START):\n",
    "        prime_str = FILE_START + prime_str\n",
    "\n",
    "    input_seq = list(prime_str)\n",
    "    # use priming sequence to construct the hidden state\n",
    "    input_tensor = charseq_to_tensor(input_seq)\n",
    "    for i in range(len(input_seq)):\n",
    "        output, hidden = rnn(input_tensor.narrow(0, i, 1), hidden)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"\".join(input_seq))\n",
    "        print(\"~~~~Predict~~~~\")\n",
    "        print(\"\".join(input_seq))\n",
    "\n",
    "    # predict until max_len or FILE_END character is reached\n",
    "    predicted = input_seq[:]\n",
    "    for i in range(max_gen_len):\n",
    "        if temperature is not None:\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            char = INT2CHAR[top_i.item()]\n",
    "        else:\n",
    "            _, pred_char_idx = output.topk(1)\n",
    "            char = INT2CHAR[pred_char_idx.item()]\n",
    "        if print_output:\n",
    "            print(char, end=\"\")\n",
    "        predicted.append(char)\n",
    "        if char == FILE_END:\n",
    "            return predicted\n",
    "        input_tensor = char_to_tensor(char)\n",
    "        output, hidden = rnn(input_tensor, hidden)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"~max_gen_len reached~\")\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002\n",
      "~~~~Predict~~~~\n",
      "\u0002\n",
      "from self.abser.mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask~max_gen_len reached~\n"
     ]
    }
   ],
   "source": [
    "output = generate(char_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you use this with your own text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002def\n",
      "~~~~Predict~~~~\n",
      "\u0002def\n",
      " insting.\n",
      "\n",
      "    def __init____init____init____init___ser__inate_== 'name', 'conting_configering', 'service', 'config', 'files')\n",
      "    if self._config_content_contest_file_insting_contest_file_idsion_layer_cord_configer, context=\n",
      "            'time', 'service', 'server')\n",
      "        self._context_contest_file_inder_contest_contest_contest_contest_cast_id, config, config_contest_file_id)\n",
      "        return self._contest_cast_contige_time\n",
      "        if self._context_contest_file_indestingent()\n",
      "            if not self.ald_case_inst_componert(self):\n",
      "                                                        return self._contion_in\n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                               ~max_gen_len reached~\n"
     ]
    }
   ],
   "source": [
    "output = generate(char_rnn, \"def\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
